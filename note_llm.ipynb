{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM extraction\n",
    "\n",
    "- develop prompt & apply_llm method\n",
    "- update extraction.ipynb\n",
    "  - (make it into library)\n",
    "  - make logging feature\n",
    "  - how long it takes\n",
    "- try on 100 papers & fix\n",
    "- extract papers from openalex (stats on how many we can extract)\n",
    "- update notion on everything\n",
    "\n",
    "---\n",
    "\n",
    "- Where I left off:\n",
    "  - try on more papers?\n",
    "  - make longer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF->HTML\n",
    "\n",
    "- ~~pyalex download pdf~~\n",
    "- ~~name without space~~\n",
    "- ~~parse pdf->html~~ (pdf2html.py doesn't work for some)\n",
    "- ~~check 100 papers on quality of output html~~\n",
    "- beautiful soup\n",
    "- integrate to algorithm\n",
    "- try on 1k papers\n",
    "- how it all works\n",
    "\n",
    "---\n",
    "\n",
    "- Where I left off:\n",
    "  - report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to talk about\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT4\n",
    "\n",
    "  - output context length limitation\n",
    "    - append outputs iteratively\n",
    "    - not smooth? (maybe not?) (between responses)\n",
    "    - doesn't guarantee json output?\n",
    "    - doesn't extract full section?\n",
    "  - json mode only outputs 1050-ish tokens?, outputs newline till token limit, unstable\n",
    "  - logging system\n",
    "  - takes quite a long time\n",
    "\n",
    "  - target all papers possible through openalex? all languages?\n",
    "  - changes to LLM library -> notion? push?\n",
    "  - context length - differs significantly. Not only\n",
    "  - how much does it cost?? doesn't show\n",
    "  - how will we run code on which device?\n",
    "  - for running on 250k USD things to consider:\n",
    "    - consider possible errors (e.g. openalex 100k api calls per day)\n",
    "    - output inconsistency\n",
    "    - gpt error\n",
    "      - RateLimitError: Error code: 429 - {'error': {'code': '429', 'message': 'Requests to the ChatCompletions_Create Operation under Azure OpenAI API version 2023-05-15 have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 48 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.'}}\n",
    "\n",
    "- pdf2html\n",
    "\n",
    "  - license for unilex-transcript\n",
    "  - cant parse some pdf -> html (format is different? Latex not pdf)\n",
    "  - output html not good for some -> section header not h tag\n",
    "    - works well for recent papers (papers with clear structure & words & font size difference) (https://pdf2htmlex.github.io/pdf2htmlEX/ / Adam)\n",
    "    - not so good for old(?) papers (A_comprehensive_set_of_sequence_analysis_programs_for_the_VAX)\n",
    "  - tried on N \"supposedly\" Open Access papers\n",
    "    -> 200/364 successful PDF retrieval\n",
    "    -> 170/200 successful PDF->HTML parse\n",
    "    -> 30/170 well structured HTML\n",
    "    - some don't detect, some detect but detects other as h tag\n",
    "    - unstable, without much pattern/consistency (only around 15%)\n",
    "\n",
    "- semantic scholars api\n",
    "\n",
    "  - can use commercially\n",
    "  - good. section divided -> but need to classify + section text not attached\n",
    "  - gives full text + start, ending index of sections\n",
    "  - 13M records? (not sure) vs 53M \"supposedly\" open access OpenAlex\n",
    "  - Semantic Scholars API -> like OpenAlex\n",
    "  - \"embeddings\" attribute from specter (s2ag)\n",
    "  - specter -> embedding model for scientific paper (document level, citation informed) (https://github.com/allenai/specter)\n",
    "\n",
    "- Others\n",
    "  - work hours. might be a bit busy in April\n",
    "  - what to do from now? (try on more papers, make pipeline/ organize code?)\n",
    "  - notion. will update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://victoria.dev/blog/how-to-send-long-text-input-to-chatgpt-using-the-openai-api/ (prompt for long text)\n",
    "- https://github.com/jupediaz/chatgpt-prompt-splitter/blob/main/api/index.py (splitting text for long file)\n",
    "- https://learn.microsoft.com/en-us/answers/questions/1193969/how-to-integrate-tiktoken-library-with-azure-opena (tiktoken for azure openai)\n",
    "- https://platform.openai.com/docs/guides/text-generation/chat-completions-api (chat completion api openai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Introduction\n",
    "   Recognizing human actions in real-world environment\n",
    "   ﬁnds applications in a variety of domains including in-\n",
    "   telligent video surveillance, customer attributes, and\n",
    "   shopping behavior analysis. However, accurate recog-\n",
    "   nition of actions is a highly challenging task due to\n",
    "   Appearing in Proceedings of the 27thInternational Confer-\n",
    "   ence on Machine Learning , Haifa, Israel, 2010. Copyright\n",
    "   2010 by the author(s)/owner(s).cluttered backgrounds, occlusions, and viewpoint vari-\n",
    "   ations, etc. Therefore, most of the existing approaches\n",
    "   (Efros et al. ,2003;Sch¨ uldt et al. ,2004;Doll´ ar et al. ,\n",
    "   2005;Laptev & P´ erez ,2007;Jhuang et al. ,2007)\n",
    "   make certain assumptions (e.g., small scale and view-\n",
    "   point changes) about the circumstances under which\n",
    "   the video was taken. However, such assumptions sel-\n",
    "   dom hold in real-world environment. In addition, most\n",
    "   of these approaches follow the conventional paradigm\n",
    "   of pattern recognition, which consists of two steps in\n",
    "   which the ﬁrst step computes complex handcrafted fea-\n",
    "   tures from raw video frames and the second step learns\n",
    "   classiﬁers based on the obtained features. In real-world\n",
    "   scenarios, it is rarely known which features are impor-\n",
    "   tant for the task at hand, since the choice of feature is\n",
    "   highly problem-dependent. Especially for human ac-\n",
    "   tion recognition, diﬀerent action classes may appear\n",
    "   dramatically diﬀerent in terms of their appearances\n",
    "   and motion patterns.\n",
    "   Deep learning models ( Fukushima ,1980;LeCun et al. ,\n",
    "   1998;Hinton & Salakhutdinov ,2006;Hinton et al. ,\n",
    "   2006;Bengio ,2009) are a class of machines that can\n",
    "   learn a hierarchy of features by building high-level\n",
    "   features from low-level ones, thereby automating the\n",
    "   process of feature construction. Such learning ma-\n",
    "   chines can be trained using either supervised or un-\n",
    "   supervised approaches, and the resulting systems have\n",
    "   been shown to yield competitive performance in visual\n",
    "   object recognition ( LeCun et al. ,1998;Hinton et al. ,\n",
    "   2006;Ranzato et al. ,2007;Lee et al. ,2009a), natu-\n",
    "   ral language processing ( Collobert & Weston ,2008),\n",
    "   and audio classiﬁcation ( Lee et al. ,2009b ) tasks. The\n",
    "   convolutional neural networks (CNNs) ( LeCun et al. ,\n",
    "\n",
    "1998) are a type of deep models in which trainable\n",
    "      ﬁlters and local neighborhood pooling operations are\n",
    "      applied alternatingly on the raw input images, result-\n",
    "      ing in a hierarchy of increasingly complex features.\n",
    "      It has been shown that, when trained with appropri-3D Convolutional Neural Networks for Human Action Recognit ion\n",
    "      ate regularization ( Ahmed et al. ,2008;Yu et al. ,2008;\n",
    "      Mobahi et al. ,2009), CNNs can achieve superior per-\n",
    "      formance on visual object recognition tasks without\n",
    "      relying on handcrafted features. In addition, CNNs\n",
    "      have been shown to be relatively insensitive to certain\n",
    "      variations on the inputs ( LeCun et al. ,2004).\n",
    "      As a class of attractive deep models for automated fea-\n",
    "      ture construction, CNNs have been primarily applied\n",
    "      on 2D images. In this paper, we consider the use of\n",
    "      CNNs for human action recognition in videos. A sim-\n",
    "      ple approach in this direction is to treat video frames\n",
    "      as still images and apply CNNs to recognize actions\n",
    "      at the individual frame level. Indeed, this approach\n",
    "      has been used to analyze the videos of developing\n",
    "      embryos ( Ning et al. ,2005). However, such approach\n",
    "      does not consider the motion information encoded in\n",
    "      multiple contiguous frames. To eﬀectively incorporate\n",
    "      the motion information in video analysis, we propose\n",
    "      to perform 3D convolution in the convolutional layers\n",
    "      of CNNs so that discriminative features along both\n",
    "      spatial and temporal dimensions are captured. We\n",
    "      show that by applying multiple distinct convolutional\n",
    "      operations at the same location on the input, multi-\n",
    "      ple types of features can be extracted. Based on the\n",
    "      proposed 3D convolution, a variety of 3D CNN archi-\n",
    "      tectures can be devised to analyze video data. We\n",
    "      develop a 3D CNN architecture that generates multi-\n",
    "      ple channels of information from adjacent video frames\n",
    "      and performs convolution and subsampling separately\n",
    "      in each channel. The ﬁnal feature representation is\n",
    "      obtained by combining information from all channels.\n",
    "      An additional advantage of the CNN-based models is\n",
    "      that the recognition phase is very eﬃcient due to their\n",
    "      feed-forward nature.\n",
    "      We evaluated the developed 3D CNN model on the\n",
    "      TREC Video Retrieval Evaluation (TRECVID) data1,\n",
    "      which consist of surveillance video data recorded in\n",
    "      London Gatwick Airport. We constructed a multi-\n",
    "      module event detection system, which includes 3D\n",
    "      CNN as a module, and participated in three tasks of\n",
    "      the TRECVID 2009 Evaluation for Surveillance Event\n",
    "      Detection. Our system achieved the best performance\n",
    "      on all three participated tasks. To provide indepen-\n",
    "      dent evaluation of the 3D CNN model, we report its\n",
    "      performance on the TRECVID 2008 development set\n",
    "      in this paper. We also present results on the KTH\n",
    "      data as published performance for this data is avail-\n",
    "      able. Our experiments show that the developed 3D\n",
    "      CNN model outperforms other baseline methods on\n",
    "      the TRECVID data, and it achieves competitive per-\n",
    "      formance on the KTH data without depending on\n",
    "      1http://www-nlpir.nist.gov/projects/trecvid/handcrafted features, demonstrating that the 3D CNN\n",
    "      model is more eﬀective for real-world environments\n",
    "      such as those captured in TRECVID data. The exper-\n",
    "      iments also show that the 3D CNN model signiﬁcantly\n",
    "      outperforms the frame-based 2D CNN for most tasks.\n",
    "      We also observe that the performance diﬀerences be-\n",
    "      tween 3D CNN and other methods tend to be larger\n",
    "      when the number of positive training samples is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 3D Convolutional Neural Networks\n",
    "   In 2D CNNs, 2D convolution is performed at the con-\n",
    "   volutional layers to extract features from local neigh-\n",
    "   borhood on feature maps in the previous layer. Then\n",
    "   an additive bias is applied and the result is passed\n",
    "   through a sigmoid function. Formally, the value of\n",
    "   unit at position ( x, y) in the jth feature map in the\n",
    "   ith layer, denoted as vxy\n",
    "   ij, is given by\n",
    "   vxy\n",
    "   ij= tanh(\n",
    "   bij+∑\n",
    "   mPi−1∑\n",
    "   p=0Qi−1∑\n",
    "   q=0wpq\n",
    "   ijmv(x+p)(y+q)\n",
    "   (i−1)m)\n",
    "   ,\n",
    "   (1)\n",
    "   where tanh( ·) is the hyperbolic tangent function, bij\n",
    "   is the bias for this feature map, mindexes over the\n",
    "   set of feature maps in the ( i−1)th layer connected\n",
    "   to the current feature map, wpq\n",
    "   ijkis the value at the\n",
    "   position ( p, q) of the kernel connected to the kth fea-\n",
    "   ture map, and PiandQiare the height and width\n",
    "   of the kernel, respectively. In the subsampling lay-\n",
    "   ers, the resolution of the feature maps is reduced by\n",
    "   pooling over local neighborhood on the feature maps\n",
    "   in the previous layer, thereby increasing invariance to\n",
    "   distortions on the inputs. A CNN architecture can be\n",
    "   constructed by stacking multiple layers of convolution\n",
    "   and subsampling in an alternating fashion. The pa-\n",
    "   rameters of CNN, such as the bias bijand the kernel\n",
    "   weight wpq\n",
    "   ijk, are usually trained using either super-\n",
    "   vised or unsupervised approaches ( LeCun et al. ,1998;\n",
    "   Ranzato et al. ,2007).\n",
    "   2.1. 3D Convolution\n",
    "   In 2D CNNs, convolutions are applied on the 2D fea-\n",
    "   ture maps to compute features from the spatial dimen-\n",
    "   sions only. When applied to video analysis problems,\n",
    "   it is desirable to capture the motion information en-\n",
    "   coded in multiple contiguous frames. To this end, we\n",
    "   propose to perform 3D convolutions in the convolution\n",
    "   stages of CNNs to compute features from both spa-\n",
    "   tial and temporal dimensions. The 3D convolution is\n",
    "   achieved by convolving a 3D kernel to the cube formed\n",
    "   by stacking multiple contiguous frames together. By\n",
    "   this construction, the feature maps in the convolution\n",
    "   layer is connected to multiple contiguous frames in the3D Convolutional Neural Networks for Human Action Recognit ion\n",
    "   (a) 2D convolution\n",
    "   t\n",
    "   e\n",
    "   m\n",
    "   p\n",
    "   o\n",
    "   r\n",
    "   a\n",
    "   l\n",
    "   (b) 3D convolution\n",
    "   Figure 1. Comparison of 2D (a) and 3D (b) convolutions.\n",
    "   In (b) the size of the convolution kernel in the temporal\n",
    "   dimension is 3, and the sets of connections are color-coded\n",
    "   so that the shared weights are in the same color. In 3D\n",
    "   convolution, the same 3D kernel is applied to overlapping\n",
    "   3D cubes in the input video to extract motion features.\n",
    "   previous layer, thereby capturing motion information.\n",
    "   Formally, the value at position ( x, y, z ) on the jth fea-\n",
    "   ture map in the ith layer is given by\n",
    "   vxyz\n",
    "   ij=tanh(\n",
    "   bij+∑\n",
    "   mPi−1∑\n",
    "   p=0Qi−1∑\n",
    "   q=0Ri−1∑\n",
    "   r=0wpqr\n",
    "   ijmv(x+p)(y+q)(z+r)\n",
    "   (i−1)m)\n",
    "   ,\n",
    "   (2)\n",
    "   where Riis the size of the 3D kernel along the tem-\n",
    "   poral dimension, wpqr\n",
    "   ijmis the ( p, q, r)th value of the\n",
    "   kernel connected to the mth feature map in the previ-\n",
    "   ous layer. A comparison of 2D and 3D convolutions is\n",
    "   given in Figure 1.\n",
    "   Note that a 3D convolutional kernel can only extract\n",
    "   one type of features from the frame cube, since the\n",
    "   kernel weights are replicated across the entire cube. A\n",
    "   general design principle of CNNs is that the number\n",
    "   of feature maps should be increased in late layers by\n",
    "   generating multiple types of features from the samet\n",
    "   e\n",
    "   m\n",
    "   p\n",
    "   o\n",
    "   r\n",
    "   a\n",
    "   l\n",
    "   Figure 2. Extraction of multiple features from contiguous\n",
    "   frames. Multiple 3D convolutions can be applied to con-\n",
    "   tiguous frames to extract multiple features. As in Figure 1,\n",
    "   the sets of connections are color-coded so that the shared\n",
    "   weights are in the same color. Note that all the 6 sets of\n",
    "   connections do not share weights, resulting in two diﬀerent\n",
    "   feature maps on the right.\n",
    "   set of lower-level feature maps. Similar to the case\n",
    "   of 2D convolution, this can be achieved by applying\n",
    "   multiple 3D convolutions with distinct kernels to the\n",
    "   same location in the previous layer (Figure 2).\n",
    "   2.2. A 3D CNN Architecture\n",
    "   Based on the 3D convolution described above, a variety\n",
    "   of CNN architectures can be devised. In the following,\n",
    "   we describe a 3D CNN architecture that we have devel-\n",
    "   oped for human action recognition on the TRECVID\n",
    "   data set. In this architecture shown in Figure 3, we\n",
    "   consider 7 frames of size 60 ×40 centered on the current\n",
    "   frame as inputs to the 3D CNN model. We ﬁrst apply a\n",
    "   set of hardwired kernels to generate multiple channels\n",
    "   of information from the input frames. This results in\n",
    "   33 feature maps in the second layer in 5 diﬀerent chan-\n",
    "   nels known as gray, gradient-x, gradient-y, optﬂow-x,\n",
    "   and optﬂow-y. The gray channel contains the gray\n",
    "   pixel values of the 7 input frames. The feature maps\n",
    "   in the gradient-x and gradient-y channels are obtained\n",
    "   by computing gradients along the horizontal and ver-\n",
    "   tical directions, respectively, on each of the 7 input\n",
    "   frames, and the optﬂow-x and optﬂow-y channels con-\n",
    "   tain the optical ﬂow ﬁelds, along the horizontal and\n",
    "   vertical directions, respectively, computed from adja-\n",
    "   cent input frames. This hardwired layer is used to en-\n",
    "   code our prior knowledge on features, and this scheme\n",
    "   usually leads to better performance as compared to\n",
    "   random initialization.3D Convolutional Neural Networks for Human Action Recognit ion\n",
    "   H1:\n",
    "   33@60x40\n",
    "   C2:\n",
    "   23*2@54x34\n",
    "   7x7x3 3D\n",
    "   convolution\n",
    "   2x2\n",
    "   subsampling\n",
    "   S3:\n",
    "   23*2@27x17\n",
    "   7x6x3 3D\n",
    "   convolution\n",
    "   C4:\n",
    "   13*6@21x12\n",
    "   3x3\n",
    "   subsampling\n",
    "   S5:\n",
    "   13*6@7x4\n",
    "   7x4\n",
    "   convolution\n",
    "   C6:\n",
    "   128@1x1\n",
    "   full\n",
    "   connnection\n",
    "   hardwired\n",
    "   input:\n",
    "   7@60x40\n",
    "   Figure 3. A 3D CNN architecture for human action recognition. This arc hitecture consists of 1 hardwired layer, 3 convo-\n",
    "   lution layers, 2 subsampling layers, and 1 full connection l ayer. Detailed descriptions are given in the text.\n",
    "   We then apply 3D convolutions with a kernel size of\n",
    "   7×7×3 (7×7 in the spatial dimension and 3 in the\n",
    "   temporal dimension) on each of the 5 channels sepa-\n",
    "   rately. To increase the number of feature maps, two\n",
    "   sets of diﬀerent convolutions are applied at each loca-\n",
    "   tion, resulting in 2 sets of feature maps in the C2 layer\n",
    "   each consisting of 23 feature maps. This layer con-\n",
    "   tains 1,480 trainable parameters. In the subsequent\n",
    "   subsampling layer S3, we apply 2 ×2 subsampling on\n",
    "   each of the feature maps in the C2 layer, which leads\n",
    "   to the same number of feature maps with reduced spa-\n",
    "   tial resolution. The number of trainable parameters in\n",
    "   this layer is 92. The next convolution layer C4 is ob-\n",
    "   tained by applying 3D convolution with a kernel size\n",
    "   of 7×6×3 on each of the 5 channels in the two sets\n",
    "   of feature maps separately. To increase the number\n",
    "   of feature maps, we apply 3 convolutions with diﬀer-\n",
    "   ent kernels at each location, leading to 6 distinct sets\n",
    "   of feature maps in the C4 layer each containing 13\n",
    "   feature maps. This layer contains 3,810 trainable pa-\n",
    "   rameters. The next layer S5 is obtained by applying\n",
    "   3×3 subsampling on each feature maps in the C4 layer,\n",
    "   which leads to the same number of feature maps with\n",
    "   reduced spatial resolution. The number of trainable\n",
    "   parameters in this layer is 156. At this stage, the size\n",
    "   of the temporal dimension is already relatively small\n",
    "   (3 for gray, gradient-x, gradient-y and 2 for optﬂow-x\n",
    "   and optﬂow-y), so we perform convolution only in the\n",
    "   spatial dimension at this layer. The size of the con-\n",
    "   volution kernel used is 7 ×4 so that the sizes of the\n",
    "   output feature maps are reduced to 1 ×1. The C6 layer\n",
    "   consists of 128 feature maps of size 1 ×1, and each of\n",
    "   them is connected to all the 78 feature maps in the S5\n",
    "   layer, leading to 289,536 trainable parameters.\n",
    "   By the multiple layers of convolution and subsampling,the 7 input frames have been converted into a 128D\n",
    "   feature vector capturing the motion information in the\n",
    "   input frames. The output layer consists of the same\n",
    "   number of units as the number of actions, and each\n",
    "   unit is fully connected to each of the 128 units in\n",
    "   the C6 layer. In this design we essentially apply a\n",
    "   linear classiﬁer on the 128D feature vector for action\n",
    "   classiﬁcation. For an action recognition problem with\n",
    "   3 classes, the number of trainable parameters at the\n",
    "   output layer is 384. The total number of trainable\n",
    "   parameters in this 3D CNN model is 295,458, and all\n",
    "   of them are initialized randomly and trained by on-\n",
    "   line error back-propagation algorithm as described in\n",
    "   (LeCun et al. ,1998). We have designed and evalu-\n",
    "   ated other 3D CNN architectures that combine mul-\n",
    "   tiple channels of information at diﬀerent stages, and\n",
    "   our results show that this architecture gives the best\n",
    "   performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Experiments\n",
    "   We perform experiments on the TRECVID 2008 data\n",
    "   and the KTH data ( Sch¨ uldt et al. ,2004) to evaluate\n",
    "   the developed 3D CNN model for action recognition.\n",
    "   4.1. Action Recognition on TRECVID Data\n",
    "   The TRECVID 2008 development data set consists of\n",
    "   49-hour videos captured at the London Gatwick Air-\n",
    "   port using 5 diﬀerent cameras with a resolution of\n",
    "   720×576 at 25 fps. The videos recorded by camera\n",
    "   number 4 are excluded as few events occurred in this\n",
    "   scene. In this experiments, we focus on the recognitionof 3 action classes ( CellToEar ,ObjectPut , andPoint-\n",
    "   ing). Each action is classiﬁed in the one-against-rest\n",
    "   manner, and a large number of negative samples were\n",
    "   generated from actions that are not in these 3 classes.\n",
    "   This data set was captured on ﬁve days (20071101,\n",
    "   20071106, 20071107, 20071108, and 20071112), and\n",
    "   the statistics of the data used in our experiments are\n",
    "   summarized in Table 1. The 3D CNN model used in\n",
    "   this experiment is as described in Section 2and Figure\n",
    "   3, and the number of training iterations are tuned on\n",
    "   a separate validation set.\n",
    "   As the videos were recorded in real-world environ-\n",
    "   ments, and each frame contains multiple humans, we\n",
    "   apply a human detector and a detection-driven tracker\n",
    "   to locate human heads. Some sample human detec-\n",
    "   tion and tracking results are shown in Figure 4. Based\n",
    "   on the detection and tracking results, a bounding box\n",
    "   for each human that performs action was computed.\n",
    "   The multiple frames required by 3D CNN model are\n",
    "   obtained by extracting bounding boxes at the same\n",
    "   position from consecutive frames before and after the\n",
    "   current frame, leading to a cube containing the ac-\n",
    "   tion. The temporal dimension of the cube is set to\n",
    "   7 in our experiments as it has been shown that 5-7\n",
    "   frames are enough to achieve a performance similar\n",
    "   to the one obtainable with the entire video sequence\n",
    "   (Schindler & Van Gool ,2008). The frames were ex-\n",
    "   tracted with a step size of 2. That is, suppose the\n",
    "   current frame is numbered 0, we extract a bounding\n",
    "   box at the same position from frames numbered -6, -4,\n",
    "   -2, 0, 2, 4, and 6. The patch inside the bounding box\n",
    "   on each frame is scaled to 60 ×40 pixels.\n",
    "   To evaluate the eﬀectiveness of the 3D CNN model, we\n",
    "   report the results of the frame-based 2D CNN model.\n",
    "   In addition, we compare the 3D CNN model with two\n",
    "   other baseline methods, which follow the state-of-the-\n",
    "   art bag-of-words (BoW) paradigm in which complex\n",
    "   handcrafted features are computed. For each image\n",
    "   cube as used in 3D CNN, we construct a BoW feature\n",
    "   based on dense local invariant features. Then a one-3D Convolutional Neural Networks for Human Action Recognit ion\n",
    "   Figure 4. Sample human detection and tracking results from camera num bers 1, 2, 3, and 5, respectively from left to right.\n",
    "   against-all linear SVM is learned for each action class.\n",
    "   Speciﬁcally, we extract dense SIFT descriptors ( Lowe,\n",
    "\n",
    "2004) from raw gray images or motion edge history\n",
    "      images (MEHI) ( Yang et al. ,2009). Local features on\n",
    "      raw gray images preserve the appearance information,\n",
    "      while MEHI concerns with the shape and motion pat-\n",
    "      terns. These SIFT descriptors are calculated every 6\n",
    "      pixels from 7 ×7 and 16 ×16 local image patches in the\n",
    "      same cubes as in the 3D CNN model. Then they are\n",
    "      softly quantized using a 512-word codebook to build\n",
    "      the BoW features. To exploit the spatial layout in-\n",
    "      formation, we employ similar approach as the spatial\n",
    "      pyramid matching (SPM) ( Lazebnik et al. ,2006) to\n",
    "      partition the candidate region into 2 ×2 and 3 ×4 cells\n",
    "      and concatenate their BoW features. The dimension-\n",
    "      ality of the entire feature vector is 512 ×(2×2+3×4) =\n",
    "\n",
    "8192. We denote the method based on gray images as\n",
    "      SPMcube\n",
    "      grayand the one based on MEHI as SPMcube\n",
    "      MEHI.\n",
    "      We report the 5-fold cross-validation results in which\n",
    "      the data for a single day are used as a fold. The per-\n",
    "      formance measures we used are precision, recall, and\n",
    "      area under the ROC curve (ACU) at multiple values of\n",
    "      false positive rates (FPR). The performance of the four\n",
    "      methods is summarized in Table 2. We can observe\n",
    "      from Table 2that the 3D CNN model outperforms the\n",
    "      frame-based 2D CNN model, SPMcube\n",
    "      gray, and SPMcube\n",
    "      MEHI\n",
    "      signiﬁcantly on the action classes CellToEar andOb-\n",
    "      jectPut in all cases. For the action class Pointing , 3D\n",
    "      CNN model achieves slightly worse performance than\n",
    "      the other three methods. From Table 1we can see that\n",
    "      the number of positive samples in the Pointing class is\n",
    "      signiﬁcantly larger than those of the other two classes.\n",
    "      Hence, we can conclude that the 3D CNN model is\n",
    "      more eﬀective when the number of positive samples is\n",
    "      small. Overall, the 3D CNN model outperforms other\n",
    "      three methods consistently as can be seen from the\n",
    "      average performance in Table 2.\n",
    "      4.2. Action Recognition on KTH Data\n",
    "      We evaluate the 3D CNN model on the KTH data\n",
    "      (Sch¨ uldt et al. ,2004), which consist of 6 action classesperformed by 25 subjects. To follow the setup in the\n",
    "      HMAX model, we use a 9-frame cube as input and ex-\n",
    "      tract foreground as in ( Jhuang et al. ,2007). To reduce\n",
    "      the memory requirement, the resolutions of the input\n",
    "      frames are reduced to 80 ×60 in our experiments as\n",
    "      compared to 160 ×120 used in ( Jhuang et al. ,2007).\n",
    "      We use a similar 3D CNN architecture as in Figure\n",
    "      3with the sizes of kernels and the number of feature\n",
    "      maps in each layer modiﬁed to consider the 80 ×60×9\n",
    "      inputs. In particular, the three convolutional layers\n",
    "      use kernels of sizes 9 ×7, 7×7, and 6 ×4, respec-\n",
    "      tively, and the two subsampling layers use kernels of\n",
    "      size 3 ×3. By using this setting, the 80 ×60×9 in-\n",
    "      puts are converted into 128D feature vectors. The ﬁnal\n",
    "      layer consists of 6 units corresponding to the 6 classes.\n",
    "      As in ( Jhuang et al. ,2007), we use the data for 16 ran-\n",
    "      domly selected subjects for training, and the data for\n",
    "      the other 9 subjects for testing. The recognition per-\n",
    "      formance averaged across 5 random trials is reported\n",
    "      in Table 3along with published results in the litera-\n",
    "      ture. The 3D CNN model achieves an overall accu-\n",
    "      racy of 90.2% as compared with 91.7% achieved by\n",
    "      the HMAX model. Note that the HMAX model use\n",
    "      handcrafted features computed from raw images with\n",
    "      4-fold higher resolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Conclusions and Discussions\n",
    "   We developed a 3D CNN model for action recognition\n",
    "   in this paper. This model construct features from both\n",
    "   spatial and temporal dimensions by performing 3D\n",
    "   convolutions. The developed deep architecture gener-\n",
    "   ates multiple channels of information from adjacent in-\n",
    "   put frames and perform convolution and subsampling\n",
    "   separately in each channel. The ﬁnal feature represen-\n",
    "   tation is computed by combining information from all\n",
    "   channels. We evaluated the 3D CNN model using the\n",
    "   TRECVID and the KTH data sets. Results show that\n",
    "   the 3D CNN model outperforms compared methods\n",
    "   on the TRECVID data, while it achieves competitive\n",
    "   performance on the KTH data, demonstrating its su-\n",
    "   perior performance in real-world environments.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
