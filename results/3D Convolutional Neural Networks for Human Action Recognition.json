{
  "filename": "3D Convolutional Neural Networks for Human Action Recognition",
  "extracted-section-list": [
    "Introduction",
    "3D Convolutional Neural Networks",
    "Related Work",
    "Experiments",
    "Conclusions and Discussions"
  ],
  "target-section-extraction-result": {
    "introduction": "Recognizing human actions in real-world environment\n­s applications in a variety of domains including in-\ntelligent video surveillance, customer attributes, and\nshopping behavior analysis. However, accurate recog-\nnition of actions is a highly challenging task due to\nAppearing in Proceedings of the 27thInternational Confer-\nence on Machine Learning , Haifa, Israel, 2010. Copyright\n2010 by the author(s)/owner(s).cluttered backgrounds, occlusions, and viewpoint vari-\nations, etc. Therefore, most of the existing approaches\n(Efros et al. ,2003;Scḧ uldt et al. ,2004;Dolĺ ar et al. ,\n2005;Laptev & Ṕ erez ,2007;Jhuang et al. ,2007)\nmake certain assumptions (e.g., small scale and view-\npoint changes) about the circumstances under which\nthe video was taken. However, such assumptions sel-\ndom hold in real-world environment. In addition, most\nof these approaches follow the conventional paradigm\nof pattern recognition, which consists of two steps in\nwhich the ­rst step computes complex handcrafted fea-\ntures from raw video frames and the second step learns\nclassi­ers based on the obtained features. In real-world\nscenarios, it is rarely known which features are impor-\ntant for the task at hand, since the choice of feature is\nhighly problem-dependent. Especially for human ac-\ntion recognition, diâerent action classes may appear\ndramatically diâerent in terms of their appearances\nand motion patterns.\nDeep learning models ( Fukushima ,1980;LeCun et al. ,\n1998;Hinton & Salakhutdinov ,2006;Hinton et al. ,\n2006;Bengio ,2009) are a class of machines that can\nlearn a hierarchy of features by building high-level\nfeatures from low-level ones, thereby automating the\nprocess of feature construction. Such learning ma-\nchines can be trained using either supervised or un-\nsupervised approaches, and the resulting systems have\nbeen shown to yield competitive performance in visual\nobject recognition ( LeCun et al. ,1998;Hinton et al. ,\n2006;Ranzato et al. ,2007;Lee et al. ,2009a), natu-\nral language processing ( Collobert & Weston ,2008),\nand audio classi­cation ( Lee et al. ,2009b ) tasks. The\nconvolutional neural networks (CNNs) ( LeCun et al. ,\n1998) are a type of deep models in which trainable\n­lters and local neighborhood pooling operations are\napplied alternatingly on the raw input images, result-\ning in a hierarchy of increasingly complex features.\nIt has been shown that, when trained with appropri-3D Convolutional Neural Networks for Human Action Recognit ion\nate regularization ( Ahmed et al. ,2008;Yu et al. ,2008;\nMobahi et al. ,2009), CNNs can achieve superior per-\nformance on visual object recognition tasks without\nrelying on handcrafted features. In addition, CNNs\nhave been shown to be relatively insensitive to certain\nvariations on the inputs ( LeCun et al. ,2004).\nAs a class of attractive deep models for automated fea-\nture construction, CNNs have been primarily applied\non 2D images. In this paper, we consider the use of\nCNNs for human action recognition in videos. A sim-\nple approach in this direction is to treat video frames\nas still images and apply CNNs to recognize actions\nat the individual frame level. Indeed, this approach\nhas been used to analyze the videos of developing\nembryos ( Ning et al. ,2005). However, such approach\ndoes not consider the motion information encoded in\nmultiple contiguous frames. To e®ectively incorporate\nthe motion information in video analysis, we propose\nto perform 3D convolution in the convolutional layers\nof CNNs so that discriminative features along both\nspatial and temporal dimensions are captured. We\nshow that by applying multiple distinct convolutional\noperations at the same location on the input, multi-\nple types of features can be extracted. Based on the\nproposed 3D convolution, a variety of 3D CNN archi-\ntectures can be devised to analyze video data. We\ndevelop a 3D CNN architecture that generates multi-\nple channels of information from adjacent video frames\nand performs convolution and subsampling separately\nin each channel. The ­nal feature representation is\nobtained by combining information from all channels.\nAn additional advantage of the CNN-based models is\nthat the recognition phase is very e³cient due to their\nfeed-forward nature.\nWe evaluated the developed 3D CNN model on the\nTREC Video Retrieval Evaluation (TRECVID) data1,\nwhich consist of surveillance video data recorded in\nLondon Gatwick Airport. We constructed a multi-\nmodule event detection system, which includes 3D\nCNN as a module, and participated in three tasks of\nthe TRECVID 2009 Evaluation for Surveillance Event\nDetection. Our system achieved the best performance\non all three participated tasks. To provide indepen-\ndent evaluation of the 3D CNN model, we report its\nperformance on the TRECVID 2008 development set\nin this paper. We also present results on the KTH\ndata as published performance for this data is avail-\nable. Our experiments show that the developed 3D\nCNN model outperforms other baseline methods on\nthe TRECVID data, and it achieves competitive per-\nformance on the KTH data without depending on\n1http://www-nlpir.nist.gov/projects/trecvid/handcrafted features, demonstrating that the 3D CNN\nmodel is more e®ective for real-world environments\nsuch as those captured in TRECVID data. The exper-\niments also show that the 3D CNN model signi­cantly\noutperforms the frame-based 2D CNN for most tasks.\nWe also observe that the performance di®erences be-\ntween 3D CNN and other methods tend to be larger\nwhen the number of positive training samples is small.",
    "method": "In 2D CNNs, 2D convolution is performed at the con-\nvolutional layers to extract features from local neigh-\nborhood on feature maps in the previous layer. Then\nan additive bias is applied and the result is passed\nthrough a sigmoid function. Formally, the value of\nunit at position ( x, y) in the jth feature map in the\nith layer, denoted as vxy\nij, is given by\nvxy\nij= tanh(\nbij+∑\nmPi−1∑\np=0Qi−1∑\nq=0wpq\nijmv(x+p)(y+q)\n(i−1)m)\n,\n(1)\nwhere tanh( ·) is the hyperbolic tangent function, bij\nis the bias for this feature map, mindexes over the\nset of feature maps in the ( i−1)th layer connected\nto the current feature map, wpq\nijkis the value at the\nposition ( p, q) of the kernel connected to the kth fea-\nture map, and PiandQiare the height and width\nof the kernel, respectively. In the subsampling lay-\ners, the resolution of the feature maps is reduced by\npooling over local neighborhood on the feature maps\nin the previous layer, thereby increasing invariance to\ndistortions on the inputs. A CNN architecture can be\nconstructed by stacking multiple layers of convolution\nand subsampling in an alternating fashion. The pa-\nrameters of CNN, such as the bias bijand the kernel\nweight wpq\nijk, are usually trained using either super-\nvised or unsupervised approaches ( LeCun et al. ,1998;\nRanzato et al. ,2007).\n2.1. 3D Convolution\nIn 2D CNNs, convolutions are applied on the 2D fea-\nture maps to compute features from the spatial dimen-\nsions only. When applied to video analysis problems,\nit is desirable to capture the motion information en-\ncoded in multiple contiguous frames. To this end, we\npropose to perform 3D convolutions in the convolution\nstages of CNNs to compute features from both spa-\ntial and temporal dimensions. The 3D convolution is\nachieved by convolving a 3D kernel to the cube formed\nby stacking multiple contiguous frames together. By\nthis construction, the feature maps in the convolution\nlayer is connected to multiple contiguous frames in the3D Convolutional Neural Networks for Human Action Recognit ion\n(a) 2D convolution\nt\ne\nm\np\no\nr\na\nl\n(b) 3D convolution\nFigure 1. Comparison of 2D (a) and 3D (b) convolutions.\nIn (b) the size of the convolution kernel in the temporal\ndimension is 3, and the sets of connections are color-coded\nso that the shared weights are in the same color. In 3D\nconvolution, the same 3D kernel is applied to overlapping\n3D cubes in the input video to extract motion features.\nprevious layer, thereby capturing motion information.\nFormally, the value at position ( x, y, z ) on the jth fea-\nture map in the ith layer is given by\nvxyz\nij=tanh(\nbij+∑\nmPi−1∑\np=0Qi−1∑\nq=0Ri−1∑\nr=0wpqr\nijmv(x+p)(y+q)(z+r)\n(i−1)m)\n,\n(2)\nwhere Riis the size of the 3D kernel along the tem-\nporal dimension, wpqr\nijmis the ( p, q, r)th value of the\nkernel connected to the mth feature map in the previ-\nous layer. A comparison of 2D and 3D convolutions is\ngiven in Figure 1.\nNote that a 3D convolutional kernel can only extract\none type of features from the frame cube, since the\nkernel weights are replicated across the entire cube. A\ngeneral design principle of CNNs is that the number\nof feature maps should be increased in late layers by\ngenerating multiple types of features from the samet\ne\nm\np\no\nr\na\nl\nFigure 2. Extraction of multiple features from contiguous\nframes. Multiple 3D convolutions can be applied to con-\ntiguous frames to extract multiple features. As in Figure 1,\nthe sets of connections are color-coded so that the shared\nweights are in the same color. Note that all the 6 sets of\nconnections do not share weights, resulting in two diâerent\nfeature maps on the right.\nset of lower-level feature maps. Similar to the case\nof 2D convolution, this can be achieved by applying\nmultiple 3D convolutions with distinct kernels to the\nsame location in the previous layer (Figure 2).\n2.2. A 3D CNN Architecture\nBased on the 3D convolution described above, a variety\nof CNN architectures can be devised. In the following,\nwe describe a 3D CNN architecture that we have devel-\noped for human action recognition on the TRECVID\ndata set. In this architecture shown in Figure 3, we\nconsider 7 frames of size 60 ×40 centered on the current\nframe as inputs to the 3D CNN model. We ­rst apply a\nset of hardwired kernels to generate multiple channels\nof information from the input frames. This results in\n33 feature maps in the second layer in 5 diâerent chan-\nnels known as gray, gradient-x, gradient-y, optâow-x,\nand optâow-y. The gray channel contains the gray\npixel values of the 7 input frames. The feature maps\nin the gradient-x and gradient-y channels are obtained\nby computing gradients along the horizontal and ver-\ntical directions, respectively, on each of the 7 input\nframes, and the optâow-x and optâow-y channels con-\ntain the optical âow ­elds, along the horizontal and\nvertical directions, respectively, computed from adja-\ncent input frames. This hardwired layer is used to en-\ncode our prior knowledge on features, and this scheme\nusually leads to better performance as compared to\nrandom initialization.3D Convolutional Neural Networks for Human Action Recognit ion\nH1:\n33@60x40\nC2:\n23*2@54x34\n7x7x3 3D\nconvolution\n2x2\nsubsampling\nS3:\n23*2@27x17\n7x6x3 3D\nconvolution\nC4:\n13*6@21x12\n3x3\nsubsampling\nS5:\n13*6@7x4\n7x4\nconvolution\nC6:\n128@1x1\nfull\nconnnection\nhardwired\ninput:\n7@60x40\nFigure 3. A 3D CNN architecture for human action recognition. This arc hitecture consists of 1 hardwired layer, 3 convo-\nlution layers, 2 subsampling layers, and 1 full connection l ayer. Detailed descriptions are given in the text.\nWe then apply 3D convolutions with a kernel size of\n7×7×3 (7×7 in the spatial dimension and 3 in the\ntemporal dimension) on each of the 5 channels sepa-\nrately. To increase the number of feature maps, two\nsets of diâerent convolutions are applied at each loca-\ntion, resulting in 2 sets of feature maps in the C2 layer\neach consisting of 23 feature maps. This layer con-\ntains 1,480 trainable parameters. In the subsequent\nsubsampling layer S3, we apply 2 ×2 subsampling on\neach of the feature maps in the C2 layer, which leads\nto the same number of feature maps with reduced spa-\ntial resolution. The number of trainable parameters in\nthis layer is 92. The next convolution layer C4 is ob-\ntained by applying 3D convolution with a kernel size\nof 7×6×3 on each of the 5 channels in the two sets\nof feature maps separately. To increase the number\nof feature maps, we apply 3 convolutions with diâer-\nent kernels at each location, leading to 6 distinct sets\nof feature maps in the C4 layer each containing 13\nfeature maps. This layer contains 3,810 trainable pa-\nrameters. The next layer S5 is obtained by applying\n3×3 subsampling on each feature maps in the C4 layer,\nwhich leads to the same number of feature maps with\nreduced spatial resolution. The number of trainable\nparameters in this layer is 156. At this stage, the size\nof the temporal dimension is already relatively small\n(3 for gray, gradient-x, gradient-y and 2 for optâow-x\nand optâow-y), so we perform convolution only in the\nspatial dimension at this layer. The size of the con-\nvolution kernel used is 7 ×4 so that the sizes of the\noutput feature maps are reduced to 1 ×1. The C6 layer\nconsists of 128 feature maps of size 1 ×1, and each of\nthem is connected to all the 78 feature maps in the S5\nlayer, leading to 289,536 trainable parameters.\nBy the multiple layers of convolution and subsampling,the 7 input frames have been converted into a 128D\nfeature vector capturing the motion information in the\ninput frames. The output layer consists of the same\nnumber of units as the number of actions, and each\nunit is fully connected to each of the 128 units in\nthe C6 layer. In this design we essentially apply a\nlinear classi­er on the 128D feature vector for action\nclassi­cation. For an action recognition problem with\n3 classes, the number of trainable parameters at the\noutput layer is 384. The total number of trainable\nparameters in this 3D CNN model is 295,458, and all\nof them are initialized randomly and trained by on-\nline error back-propagation algorithm as described in\n(LeCun et al. ,1998). We have designed and evalu-\nated other 3D CNN architectures that combine mul-\ntiple channels of information at diâerent stages, and\nour results show that this architecture gives the best\nperformance.",
    "result": "We perform experiments on the TRECVID 2008 data\nand the KTH data ( Scḧ uldt et al. ,2004) to evaluate\nthe developed 3D CNN model for action recognition.\n4.1. Action Recognition on TRECVID Data\nThe TRECVID 2008 development data set consists of\n49-hour videos captured at the London Gatwick Air-\nport using 5 diâerent cameras with a resolution of\n720×576 at 25 fps. The videos recorded by camera\nnumber 4 are excluded as few events occurred in this\nscene. In this experiments, we focus on the recognitionof 3 action classes ( CellToEar ,ObjectPut , andPoint-\ning). Each action is classi­ed in the one-against-rest\nmanner, and a large number of negative samples were\ngenerated from actions that are not in these 3 classes.\nThis data set was captured on ­ve days (20071101,\n20071106, 20071107, 20071108, and 20071112), and\nthe statistics of the data used in our experiments are\nsummarized in Table 1. The 3D CNN model used in\nthis experiment is as described in Section 2and Figure\n3, and the number of training iterations are tuned on\na separate validation set.\nAs the videos were recorded in real-world environ-\nments, and each frame contains multiple humans, we\napply a human detector and a detection-driven tracker\nto locate human heads. Some sample human detec-\ntion and tracking results are shown in Figure 4. Based\non the detection and tracking results, a bounding box\nfor each human that performs action was computed.\nThe multiple frames required by 3D CNN model are\nobtained by extracting bounding boxes at the same\nposition from consecutive frames before and after the\ncurrent frame, leading to a cube containing the ac-\ntion. The temporal dimension of the cube is set to\n7 in our experiments as it has been shown that 5-7\nframes are enough to achieve a performance similar\nto the one obtainable with the entire video sequence\n(Schindler & Van Gool ,2008). The frames were ex-\ntracted with a step size of 2. That is, suppose the\ncurrent frame is numbered 0, we extract a bounding\nbox at the same position from frames numbered -6, -4,\n-2, 0, 2, 4, and 6. The patch inside the bounding box\non each frame is scaled to 60 ×40 pixels.\nTo evaluate the e®ectiveness of the 3D CNN model, we\nreport the results of the frame-based 2D CNN model.\nIn addition, we compare the 3D CNN model with two\nother baseline methods, which follow the state-of-the-\nart bag-of-words (BoW) paradigm in which complex\nhandcrafted features are computed. For each image\ncube as used in 3D CNN, we construct a BoW feature\nbased on dense local invariant features. Then a one-3D Convolutional Neural Networks for Human Action Recognit ion\nagainst-all linear SVM is learned for each action class.\nSpeci­cally, we extract dense SIFT descriptors ( Lowe,\n2004) from raw gray images or motion edge history\nimages (MEHI) ( Yang et al. ,2009). Local features on\nraw gray images preserve the appearance information,\nwhile MEHI concerns with the shape and motion pat-\nterns. These SIFT descriptors are calculated every 6\npixels from 7 ×7 and 16 ×16 local image patches in the\nsame cubes as in the 3D CNN model. Then they are\nsoftly quantized using a 512-word codebook to build\nthe BoW features. To exploit the spatial layout in-\nformation, we employ similar approach as the spatial\npyramid matching (SPM) ( Lazebnik et al. ,2006) to\npartition the candidate region into 2 ×2 and 3 ×4 cells\nand concatenate their BoW features. The dimension-\nality of the entire feature vector is 512 ×(2×2+3×4) =\n8192. We denote the method based on gray images as\nSPMcube\ngreyand the one based on MEHI as SPMcube\nMEHI.\nWe report the 5-fold cross-validation results in which\nthe data for a single day are used as a fold. The per-\nformance measures we used are precision, recall, and\narea under the ROC curve (ACU) at multiple values of\nfalse positive rates (FPR). The performance of the four\nmethods is summarized in Table 2. We can observe\nfrom Table 2that the 3D CNN model outperforms the\nframe-based 2D CNN model, SPMcube\ngrey, and SPMcube\nMEHI\nsigni­cantly on the action classes CellToEar andOb-\njectPut in all cases. For the action class Pointing , 3D\nCNN model achieves slightly worse performance than\nthe other three methods. From Table 1we can see that\nthe number of positive samples in the Pointing class is\nsigni­cantly larger than those of the other two classes.\nHence, we can conclude that the 3D CNN model is\nmore e®ective when the number of positive samples is\nsmall. Overall, the 3D CNN model outperforms other\nthree methods consistently as can be seen from the\naverage performance in Table 2.\n4.2. Action Recognition on KTH Data\nWe evaluate the 3D CNN model on the KTH data\n(Scḧ uldt et al. ,2004), which consist of 6 action classes\nperformed by 25 subjects. To follow the setup in the\nHMAX model, we use a 9-frame cube as input and ex-\ntract foreground as in ( Jhuang et al. ,2007). To reduce\nthe memory requirement, the resolutions of the input\nframes are reduced to 80 ×60 in our experiments as\ncompared to 160 ×120 used in ( Jhuang et al. ,2007).\nWe use a similar 3D CNN architecture as in Figure\n3with the sizes of kernels and the number of feature\nmaps in each layer modi­cations to consider the 80 ×60×9\ninputs. In particular, the three convolutional layers\nuse kernels of sizes 9 ×7, 7×7, and 6 ×4, respec-\ntively, and the two subsampling layers use kernels of\nsize 3 ×3. By using this setting, the 80 ×60×9 in-\nputs are converted into 128D feature vectors. The ­nal\nlayer consists of 6 units corresponding to the 6 classes.\nAs in ( Jhuang et al. ,2007), we use the data for 16 ran-\ndomly selected subjects for training, and the data for\nthe other 9 subjects for testing. The recognition per-\nformance averaged across 5 random trials is reported\nin Table 3along with published results in the litera-\nture. The 3D CNN model achieves an overall accu-\nracy of 90.2% as compared with 91.7% achieved by\nthe HMAX model. Note that the HMAX model use\nhandcrafted features computed from raw images with\n4-fold higher resolution.",
    "conclusion": "We developed a 3D CNN model for action recognition\nin this paper. This model construct features from both\nspatial and temporal dimensions by performing 3D\nconvolutions. The developed deep architecture gener-\nates multiple channels of information from adjacent in-\nput frames and perform convolution and subsampling\nseparately in each channel. The ­nal feature represen-\ntation is computed by combining information from all\nchannels. We evaluated the 3D CNN model using the\nTRECVID and the KTH data sets. Results show that\nthe 3D CNN model outperforms compared methods\non the TRECVID data, while it achieves competitive\nperformance on the KTH data, demonstrating its su-\nperior performance in real-world environments."
  }
}