{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALSE\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1KunlZNe9InDjmYCOpgj4wgIAScObBxh237bCOpUcUCQ',\n",
       " 'updatedRange': 'Sheet1!C10',\n",
       " 'updatedRows': 1,\n",
       " 'updatedColumns': 1,\n",
       " 'updatedCells': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to the service account\n",
    "gc = gspread.service_account(filename=\"../creds.json\")\n",
    "\n",
    "sh = gc.open(\"Paper Extraction Evaluation\").sheet1\n",
    "sh.update_cell(10, 3, \"helloooo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "from util import extract_doi_to_txt, get_string_from_text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"Results|Methodology|Methods\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 28), match='The rain in Spain. The Spain'>\n",
      "['Spain', 'Spain']\n"
     ]
    }
   ],
   "source": [
    "txt = \"The rain in Spain. The Spain\"\n",
    "x = re.search(\"^The.*Spain$\", txt)\n",
    "print(x)\n",
    "y = re.findall(\"Spain\", txt)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***Last line should not contain \"|\"***\n",
    "regex_method = r\"\"\"\n",
    "    ^\\d?[.]?\\s?Method(s)?(ology)?$|# \"method\" \"methods\" \"methodology\"\n",
    "    ^\\d?[.]?\\s?Material(s)?((\\s)and(\\s)Methods)?$ # \"materials and methods\" \"material\" \"materials\"\n",
    "\"\"\"\n",
    "\n",
    "# ***Last line should not contain \"|\"***\n",
    "regex_result_only = r\"\"\"\n",
    "    ^\\d?[.]?\\s?Result(s)?((\\s)and(\\s)discussion)?$ # \"result\" \"results\" \"results and discussion\"\n",
    "\"\"\"\n",
    "# ***Last line should not contain \"|\"***\n",
    "regex_result_others = r\"\"\"\n",
    "    ^\\d?[.]?\\s?Experiment(s)?$| # \"experiment\" \"experiments\"\n",
    "    ^\\d?[.]?\\s?Evaluation(s)?((\\s)and(\\s)comparison(s)?)?$ # \"evaluation\" \"evaluations\" \"evaluation and comparisons\"\n",
    "\"\"\"\n",
    "\n",
    "# ***Last line should not contain \"|\"***\n",
    "regex_extra = r\"\"\"\n",
    "    ^\\d?[.]?\\s?Discussion(s)?$| # \"discussion\" \"discussions\"\n",
    "    ^\\d?[.]?\\s?Reference(s)?$| # \"reference\" \"references\"\n",
    "    ^\\d?[.]?\\s?Conclusion$| # \"conclusion\"\n",
    "    ^\\d?[.]?\\s?Literature(\\s)Review$| # \"literature review\"\n",
    "    ^\\d?[.]?\\s?Introduction$| # \"introduction\"\n",
    "    ^\\d?[.]?\\s?Abstract$| # \"abstract\"\n",
    "    ^\\d?[.]?\\s?Limitation(s)?$| # \"limitation\" \"limitations\"\n",
    "    ^\\d?[.]?\\s?Appendi(x)?(ces)?$| # \"appendix\" \"appendices\"\n",
    "    ^\\d?[.]?\\s?Bibliography$| # \"bibliography\"\n",
    "    ^\\d?[.]?\\s?Acknowledgement(s)?$| # \"acknowledgement\" \"acknowledgements\"\n",
    "    ^\\d?[.]?\\s?Tables(\\s)and(\\s)Figures$ # \"tables and figures\"\n",
    "\"\"\"\n",
    "\n",
    "regex_total = (\n",
    "    regex_method\n",
    "    + \"|\"\n",
    "    + regex_result_only\n",
    "    + \"|\"\n",
    "    + regex_result_others\n",
    "    + \"|\"\n",
    "    + regex_extra\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_regex = \"hi\"\n",
    "custom_regex = None\n",
    "if custom_regex:\n",
    "    regex_total += \"\\n    \"\n",
    "    if custom_regex[0] != \"|\":\n",
    "        custom_regex = \"|\" + custom_regex\n",
    "    regex_total += custom_regex\n",
    "# print(regex_total)\n",
    "\n",
    "# add custom rules\n",
    "regex_total = re.compile(\n",
    "    regex_total,\n",
    "    re.IGNORECASE | re.VERBOSE | re.MULTILINE,\n",
    ")\n",
    "regex_method = re.compile(regex_method, re.IGNORECASE | re.VERBOSE)\n",
    "regex_result_others = re.compile(regex_result_others, re.IGNORECASE | re.VERBOSE)\n",
    "regex_result_only = re.compile(regex_result_only, re.IGNORECASE | re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 8), match='4methods'>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_method.search(\"4methods\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from document.txt...\n",
      "string successfully retrieved.\n",
      "\n",
      "<re.Match object; span=(297, 305), match='Abstract'>\n",
      "['Abstract']\n",
      "<re.Match object; span=(1578, 1590), match='Introduction'>\n",
      "['Introduction']\n",
      "<re.Match object; span=(10302, 10313), match='Methodology'>\n",
      "['Methodology']\n",
      "<re.Match object; span=(22651, 22662), match='Experiments'>\n",
      "['Experiments']\n",
      "<re.Match object; span=(25458, 25465), match='Results'>\n",
      "['Results']\n",
      "<re.Match object; span=(31333, 31343), match='Conclusion'>\n",
      "['Conclusion']\n",
      "<re.Match object; span=(32624, 32634), match='References'>\n",
      "['References']\n",
      "2\n",
      "4\n",
      "[297, 1578, 10302, 22651, 25458, 31333, 32624]\n"
     ]
    }
   ],
   "source": [
    "text = get_string_from_text_file()\n",
    "\n",
    "method_idx = -1\n",
    "result_idx = -1\n",
    "result_flag = False  # because \"Results\" section can be called many names\n",
    "section_start_point_list = []\n",
    "for i, match in enumerate(regex_total.finditer(text)):\n",
    "    section = match.group()\n",
    "    if regex_method.search(section):\n",
    "        method_idx = i\n",
    "\n",
    "    # If there is an exact match with \"Results\"\n",
    "    if regex_result_only.search(section):  # if there is an exact match\n",
    "        result_idx = i\n",
    "        result_flag = True\n",
    "    # If not, and if exact match has not been found yet, look for other names\n",
    "    if regex_result_others.search(section) and not result_flag:\n",
    "        result_idx = i\n",
    "\n",
    "    section_start_point_list.append(match.start())\n",
    "    print(match)\n",
    "    print([match.group()])\n",
    "    # break\n",
    "\n",
    "\n",
    "print(method_idx)\n",
    "print(result_idx)\n",
    "print(section_start_point_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_text = text[\n",
    "    section_start_point_list[method_idx] : section_start_point_list[method_idx + 1]\n",
    "]\n",
    "result_text = text[\n",
    "    section_start_point_list[result_idx] : section_start_point_list[result_idx + 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Results\\nExperiment Results on the ImageNet Dataset\\nThe main results are illustrated in Figure 3. The images\\npositioned on the left with red boxes represent the ground\\ntruth images. The second images from the left represent the\\nsaliency map reconstructed from EEG signals. The three im-\\nages on the right exhibit the three sampling results for the\\ngiven pixel-level saliency map with the guidance of sample-\\nlevel semantics of EEG signals. Upon comparison with the\\nground truth images and the reconstructed saliency maps,\\nwe validate that our pixel-level semantics extraction from\\nEEG signals successfully captures the color, positional, and\\nshape information of viewed images, despite limited seman-\\ntic accuracy. Comparing the GT images and three recon-\\nstructed samples, it is demonstrated that the latent diffu-\\nsion model successfully polishes the decoded saliency mapGT images Sample1 Sample2 Sample3 Saliency Map GT images Sample1 Sample2 Sample3 Saliency MapFigure 3: The main results of our N EURO IMAGEN . The images positioned on the left with red boxes represent the ground truth\\nimages. The second images from the left represent the pixel-level saliency map reconstructed from EEG signals. The three\\nimages on the right exhibit the three sampling results for the given saliency map under the guidance of sample-level semantics.\\nwith coarse-grained but accurate guidance of sample-level\\nsemantics from EEG signals. The high-quality reconstructed\\nimages purely from brain signals are perceptually and se-\\nmantically similar to the viewed images.\\nModel ACC (%) IS SSIM\\nBrain2Image 5.01\\nNeuroVision 5.23\\nNEURO IMAGEN 85.6 33.50 0.249\\nTable 1: The quantitative results of our N EURO IMAGEN ,\\nBrain2Image (Kavasidis et al. 2017) and NeuroVision\\n(Khare et al. 2022) on EEG-image dataset.\\nComparison with Baselines\\nThe quantitative results of N EURO IMAGEN and baselines\\nare listed in Table 1. We have introduced the IS reported\\nin the relevant literature, to exemplify the quality of the\\nreconstructed images. The IS is calculated by encompass-\\ning all images reconstructed across all subjects and all\\nclasses within the test set. As is demonstrated in Table 1,\\nthe IS of our N EURO IMAGEN is significantly higher than\\nBrain2Image and NeuroVision. Furthermore, inspired by\\n(Bai et al. 2023), we provide a qualitative comparison with\\nthe baselines in Figure 4. As can be seen, the quality of the\\nimages reconstructed by our N EURO IMAGEN is markedly\\nhigher than those reconstructed by the Brain2Image. This\\nobserved enhancement serves to validate the effectiveness\\nand superiority of our proposed methodology.Subject ACC (%) IS SSIM\\nsubj 01 83.84 32.64 0.254\\nsubj 02 84.26 32.33 0.247\\nsubj 03 86.66 32.93 0.251\\nsubj 04 86.48 32.40 0.244\\nsubj 05 87.62 32.97 0.250\\nsubj 06 85.25 31.76 0.245\\nTable 2: The quantitative results of different subjects.\\nGeneration Consistency in Different Subjects\\nSince EEG signals are subject-specific cognitive processes\\nthat differ significantly in different subjects. In this section,\\nwe validate the robustness and feasibility of N EURO IMA-\\nGEN across different individuals. As is illustrated in Fig-\\nure 5. The quantitative metric of different subjects are stable,\\nwhich proves the generalization ability of N EURO IMAGEN .\\nThe qualitative results are shown in Figure 5. It can be seen\\nthe sampling from different subjects are semantically similar\\nto the ground truth images.\\nAblation Study\\nWe further conduct experiments on the EEG-image dataset\\nto analyze the effectiveness of each module of our N EU-\\nROIMAGEN . We define BandLas the sample-level seman-\\ntics from EEG signals using BLIP caption as supervision or\\nlabel caption as supervision. We define Ias the pixel seman-\\ntics from EEG signals. The effectiveness of different meth-\\nods is verified by employing ACC, IS, and SSIM.Brain2Image Ours\\nAirliner Panda Jack -o’-LanternFigure 4: Comparison baseline Brain2Image (Kavasidis et al. 2017) and our proposed N EURO IMAGEN in three classes, namely\\n’Airliner’, ’Panda’, and ’Jack-o’-Lantern’. The first and second row depicts the results of Brain2Image and our N EURO IMAGEN ,\\nrespectively.\\nModel B L I ACC(%) IS SSIM\\n1%%! 4.5 16.31 0.234\\n2%!% 85.9 34.12 0.180\\n3!%% 74.1 29.87 0.157\\n4!%! 65.3 25.86 0.235\\n5%!! 85.6 33.50 0.249\\nTable 3: Quantitative results of ablation studies. BandL\\nrepresent the semantic decoding using BLIP caption and la-\\nbel caption from EEG signals, respectively. Irepresents the\\nperceptual information decoding from EEG signals.\\nPixel-level Semantics To demonstrate the effectiveness of\\nthe pixel-level semantics from EEG signals, we conduct val-\\nidation on models 2, 3, 4, and 5. By comparing 2 with 5 and\\n3 with 4, we find that using the pixel-level semantics, i.e.the\\nsaliency map, can significantly increase the structure simi-\\nlarity of the reconstructed images and ground truth images.\\nSample-level Semantics We investigate the module of\\nsample-level semantics decoding from EEG signals on guid-ing the denoising process. Models 1, 4, and 5 represent\\nthe experimental results only using the saliency, both the\\nsaliency map and sample-level semantics with the supervi-\\nsion of BLIP caption, and both the saliency map and sample-\\nlevel semantics with the supervision of label caption, respec-\\ntively. By comparing 1 with 4 and 1 with 5, the experimen-\\ntal results demonstrate that the use of sample-level seman-\\ntics significantly increases the semantic accuracy of recon-\\nstructed images.\\nBLIP Captions vs Label Captions We also compare the\\ntwo caption supervision methods with models 2 with 3 and\\n4 with 5. The experimental results of the label caption in all\\nmetrics are superior to using BLIP caption. We impute these\\nresults to that the EEG signals may only capture the class-\\nlevel information. So the prediction of BLIP latent is inaccu-\\nrate, which decreases the performance of diffusion models.\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper-extraction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
