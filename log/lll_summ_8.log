```json
{
  "filename": "3D Convolutional Neural Networks for Human Action Recognition",
  "extracted-section-list": [
    "Introduction",
    "3D Convolutional Neural Networks",
    "Related Work",
    "Experiments",
    "Conclusions and Discussions"
  ],
  "target-section-extraction-result": {
    "introduction": "Recognizing human actions in real-world environment finds applications in a variety of domains including intelligent video surveillance, customer attributes, and shopping behavior analysis. However, accurate recognition of actions is a highly challenging task due to cluttered backgrounds, occlusions, and viewpoint variations, etc. Therefore, most of the existing approaches (Efros et al. ,2003;Sch\u00c3\u00bcldt et al. ,2004;Doll\u00c3\u00a1r et al. , 2005;Laptev & P\u00c3\u00a9rez ,2007;Jhuang et al. ,2007) make certain assumptions (e.g., small scale and viewpoint changes) about the circumstances under which the video was taken. However, such assumptions seldom hold in real-world environment. In addition, most of these approaches follow the conventional paradigm of pattern recognition, which consists of two steps in which the first step computes complex handcrafted features from raw video frames and the second step learns classifiers based on the obtained features. In real-world scenarios, it is rarely known which features are important for the task at hand, since the choice of feature is highly problem-dependent. Especially for human action recognition, different action classes may appear dramatically different in terms of their appearances and motion patterns. Deep learning models ( Fukushima ,1980;LeCun et al. , 1998;Hinton & Salakhutdinov ,2006;Hinton et al. , 2006;Bengio ,2009) are a class of machines that can learn a hierarchy of features by building high-level features from low-level ones, thereby automating the process of feature construction. Such learning machines can be trained using either supervised or unsupervised approaches, and the resulting systems have been shown to yield competitive performance in visual object recognition ( LeCun et al. ,1998;Hinton et al. , 2006;Ranzato et al. ,2007;Lee et al. ,2009a), natural language processing ( Collobert & Weston ,2008), and audio classification ( Lee et al. ,2009b ) tasks. The convolutional neural networks (CNNs) ( LeCun et al. , 1998) are a type of deep models in which trainable filters and local neighborhood pooling operations are applied alternatingly on the raw input images, resulting in a hierarchy of increasingly complex features. It has been shown that, when trained with appropriate regularization ( Ahmed et al. ,2008;Yu et al. ,2008; Mobahi et al. ,2009), CNNs can achieve superior performance on visual object recognition tasks without relying on handcrafted features. In addition, CNNs have been shown to be relatively insensitive to certain variations on the inputs ( LeCun et al. ,2004). As a class of attractive deep models for automated feature construction, CNNs have been primarily applied on 2D images. In this paper, we consider the use of CNNs for human action recognition in videos. A simple approach in this direction is to treat video frames as still images and apply CNNs to recognize actions at the individual frame level. Indeed, this approach has been used to analyze the videos of developing embryos ( Ning et al. ,2005). However, such approach does not consider the motion information encoded in multiple contiguous frames. To effectively incorporate the motion information in video analysis, we propose to perform 3D convolution in the convolutional layers of CNNs so that discriminative features along both spatial and temporal dimensions are captured. We show that by applying multiple distinct convolutional operations at the same location on the input, multiple types of features can be extracted. Based on the proposed 3D convolution, a variety of 3D CNN architectures can be devised to analyze video data. We develop a 3D CNN architecture that generates multiple channels of information from adjacent video frames and performs convolution and subsampling separately in each channel. The final feature representation is obtained by combining information from all channels. An additional advantage of the CNN-based models is that the recognition phase is very efficient due to their feed-forward nature. We evaluated the developed 3D CNN model on the TREC Video Retrieval Evaluation (TRECVID) data1, which consist of surveillance video data recorded in London Gatwick Airport. We constructed a multi-module event detection system, which includes 3D CNN as a module, and participated in three tasks of the TRECVID 2009 Evaluation for Surveillance Event Detection. Our system achieved the best performance on all three participated tasks. To provide independent evaluation of the 3D CNN model, we report its performance on the TRECVID 2008 development set in this paper. We also present results on the KTH data as published performance for this data is available. Our experiments show that the developed 3D CNN model outperforms other baseline methods on the TRECVID data, and it achieves competitive performance on the KTH data without depending on handcrafted features, demonstrating that the 3D CNN model is more effective for real-world environments such as those captured in TRECVID data. The experiments also show that the 3D CNN model significantly outperforms the frame-based 2D CNN for most tasks. We also observe that the performance differences between 3D CNN and other methods tend to be larger when the number of positive training samples is small.",
    "method": "In 2D CNNs, 2D convolution is performed at the convolutional layers to extract features from local neighborhood on feature maps in the previous layer. Then an additive bias is applied and the result is passed through a sigmoid function. Formally, the value of unit at position ( x, y) in the jth feature map in the ith layer, denoted as vxy ij, is given by vxy ij= tanh( bij+∑ mPi−1∑ p=0Qi−1∑ q=0wpq ijmv(x+p)(y+q) (i−1)m) , (1) where tanh( ·) is the hyperbolic tangent function, bij is the bias for this feature map, mindexes over the set of feature maps in the ( i−1)th layer connected to the current feature map, wpq ijkis the value at the position ( p, q) of the kernel connected to the kth feature map, and PiandQiare the height and width of the kernel, respectively. In the subsampling layers, the resolution of the feature maps is reduced by pooling over local neighborhood on the feature maps in the previous layer, thereby increasing invariance to distortions on the inputs. A CNN architecture can be constructed by stacking multiple layers of convolution and subsampling in an alternating fashion. The parameters of CNN, such as the bias bijand the kernel weight wpq ijk, are usually trained using either supervised or unsupervised approaches ( LeCun et al. ,1998; Ranzato et al. ,2007). 2.1. 3D Convolution In 2D CNNs, convolutions are applied on the 2D feature maps to compute features from the spatial dimensions only. When applied to video analysis problems, it is desirable to capture the motion information encoded in multiple contiguous frames. To this end, we propose to perform 3D convolutions in the convolution stages of CNNs to compute features from both spatial and temporal dimensions. The 3D convolution is achieved by convolving a 3D kernel to the cube formed by stacking multiple contiguous frames together. By this construction, the feature maps in the convolution layer is connected to multiple contiguous frames in the previous layer, thereby capturing motion information. Formally, the value at position ( x, y, z ) on the jth feature map in the ith layer is given by vxyz ij=tanh( bij+∑ mPi−1∑ p=0Qi−1∑ q=0Ri−1∑ r=0wpqr ijmv(x+p)(y+q)(z+r) (i−1)m) , (2) where Riis the size of the 3D kernel along the temporal dimension, wpqr ijmis the ( p, q, r)th value of the kernel connected to the mth feature map in the previous layer. A comparison of 2D and 3D convolutions is given in Figure 1. Note that a 3D convolutional kernel can only extract one type of features from the frame cube, since the kernel weights are replicated across the entire cube. A general design principle of CNNs is that the number of feature maps should be increased in late layers by generating multiple types of features from the same set of lower-level feature maps. Similar to the case of 2D convolution, this can be achieved by applying multiple 3D convolutions with distinct kernels to the same location in the previous layer (Figure 2). 2.2. A 3D CNN Architecture Based on the 3D convolution described above, a variety of CNN architectures can be devised. In the following, we describe a 3D CNN architecture that we have developed for human action recognition on the TRECVID data set. In this architecture shown in Figure 3, we consider 7 frames of size 60 ×40 centered on the current frame as inputs to the 3D CNN model. We first apply a set of hardwired kernels to generate multiple channels of information from the input frames. This results in 33 feature maps in the second layer in 5 different channels known as gray, gradient-x, gradient-y, optflow-x, and optflow-y. The gray channel contains the gray pixel values of the 7 input frames. The feature maps in the gradient-x and gradient-y channels are obtained by computing gradients along the horizontal and vertical directions, respectively, on each of the 7 input frames, and the optflow-x and optflow-y channels contain the optical flow fields, along the horizontal and vertical directions, respectively, computed from adjacent input frames. This hardwired layer is used to encode our prior knowledge on features, and this scheme usually leads to better performance as compared to random initialization. We then apply 3D convolutions with a kernel size of 7×7×3 (7×7 in the spatial dimension and 3 in the temporal dimension) on each of the 5 channels separately. To increase the number of feature maps, two sets of different convolutions are applied at each location, resulting in 2 sets of feature maps in the C2 layer each consisting of 23 feature maps. This layer contains 1,480 trainable parameters. In the subsequent subsampling layer S3, we apply 2 ×2 subsampling on each of the feature maps in the C2 layer, which leads to the same number of feature maps with reduced spatial resolution. The number of trainable parameters in this layer is 92. The next convolution layer C4 is obtained by applying 3D convolution with a kernel size of 7×6×3 on each of the 5 channels in the two sets of feature maps separately. To increase the number of feature maps, we apply 3 convolutions with different kernels at each location, leading to 6 distinct sets of feature maps in the C4 layer each containing 13 feature maps. This layer contains 3,810 trainable parameters. The next layer S5 is obtained by applying 3×3 subsampling on each feature maps in the C4 layer, which leads to the same number of feature maps with reduced spatial resolution. The number of trainable parameters in this layer is 156. At this stage, the size of the temporal dimension is already relatively small (3 for gray, gradient-x, gradient-y and 2 for optflow-x and optflow-y), so we perform convolution only in the spatial dimension at this layer. The size of the convolution kernel used is 7 ×4 so that the sizes of the output feature maps are reduced to 1 ×1. The C6 layer consists of 128 feature maps of size 1 ×1, and each of them is connected to all the 78 feature maps in the S5 layer, leading to 289,536 trainable parameters. By the multiple layers of convolution and subsampling, the 7 input frames have been converted into a 128D feature vector capturing the motion information in the input frames. The output layer consists of the same number of units as the number of actions, and each unit is fully connected to each of the 128 units in the C6 layer. In this design we essentially apply a linear classifier on the 128D feature vector for action classification. For an action recognition problem with 3 classes, the number of trainable parameters at the output layer is 384. The total number of trainable parameters in this 3D CNN model is 295,458, and all of them are initialized randomly and trained by online error back-propagation algorithm as described in (LeCun et al. ,1998). We have designed and evaluated other 3D CNN architectures that combine multiple channels of information at different stages, and our results show that this architecture gives the best performance.",
    "result": "We perform experiments on the TRECVID 2008 data and the KTH data ( Sch\u00c3\u00bcldt et al. ,2004) to evaluate the developed 3D CNN model for action recognition. 4.1. Action Recognition on TRECVID Data The TRECVID 2008 development data set consists of 49-hour videos captured at the London Gatwick Airport using 5 different cameras with a resolution of 720\u00d7576 at 25 fps. The videos recorded by camera number 4 are excluded as few events occurred in this scene. In this experiments, we focus on the recognition of 3 action classes ( CellToEar ,ObjectPut , andPoint- ing). Each action is classified in the one-against-rest manner, and a large number of negative samples were generated from actions that are not in these 3 classes. This data set was captured on five days (20071101, 20071106, 20071107, 20071108, and 20071112), and the statistics of the data used in our experiments are summarized in Table 1. The 3D CNN model used in this experiment is as described in Section 2and Figure 3, and the number of training iterations are tuned on a separate validation set. As the videos were recorded in real-world environments, and each frame contains multiple humans, we apply a human detector and a detection-driven tracker to locate human heads. Some sample human detection and tracking results are shown in Figure 4. Based on the detection and tracking results, a bounding box for each human that performs action was computed. The multiple frames required by 3D CNN model are obtained by extracting bounding boxes at the same position from consecutive frames before and after the current frame, leading to a cube containing the action. The temporal dimension of the cube is set to 7 in our experiments as it has been shown that 5-7 frames are enough to achieve a performance similar to the one obtainable with the entire video sequence (Schindler & Van Gool ,2008). The frames were extracted with a step size of 2. That is, suppose the current frame is numbered 0, we extract a bounding box at the same position from frames numbered -6, -4, -2, 0, 2, 4, and 6. The patch inside the bounding box on each frame is scaled to 60 \u00d740 pixels. To evaluate the effectiveness of the 3D CNN model, we report the results of the frame-based 2D CNN model. In addition, we compare the 3D CNN model with two other baseline methods, which follow the state-of-the-art bag-of-words (BoW) paradigm in which complex handcrafted features are computed. For each image cube as used in 3D CNN, we construct a BoW feature based on dense local invariant features. Then a one-against-all linear SVM is learned for each action class. Specifically, we extract dense SIFT descriptors ( Lowe, 2004) from raw gray images or motion edge history images (MEHI) ( Yang et al. ,2009). Local features on raw gray images preserve the appearance information, while MEHI concerns with the shape and motion patterns. These SIFT descriptors are calculated every 6 pixels from 7 \u00d77 and 16 \u00d716 local image patches in the same cubes as in the 3D CNN model. Then they are softly quantized using a 512-word codebook to build the BoW features. To exploit the spatial layout information, we employ similar approach as the spatial pyramid matching (SPM) ( Lazebnik et al. ,2006) to partition the candidate region into 2 \u00d72 and 3 \u00d74 cells and concatenate their BoW features. The dimensionality of the entire feature vector is 512 \u00d7(2\u00d72+3\u00d74) = 8192. We denote the method based on gray images as SPMcube grayand the one based on MEHI as SPMcube MEHI. We report the 5-fold cross-validation results in which the data for a single day are used as a fold. The performance measures we used are precision, recall, and area under the ROC curve (ACU) at multiple values of false positive rates (FPR). The performance of the four methods is summarized in Table 2. We can observe from Table 2that the 3D CNN model outperforms the frame-based 2D CNN model, SPMcube gray, and SPMcube MEHI significantly on the action classes CellToEar andOb- jectPut in all cases. For the action class Pointing , 3D CNN model achieves slightly worse performance than the other three methods. From Table 1we can see that the number of positive samples in the Pointing class is significantly larger than those of the other two classes. Hence, we can conclude that the 3D CNN model is more effective when the number of positive samples is small. Overall, the 3D CNN model outperforms other three methods consistently as can be seen from the average performance in Table 2. 4.2. Action Recognition on KTH Data We evaluate the 3D CNN model on the KTH data (Sch\u00c3\u00bcldt et al. ,2004), which consist of 6 action classes performed by 25 subjects. To follow the setup in the HMAX model, we use a 9-frame cube as input and extract foreground as in ( Jhuang et al. ,2007). To reduce the memory requirement, the resolutions of the input frames are reduced to 80 \u00d760 in our experiments as compared to 160 \u00d7120 used in ( Jhuang et al. ,2007). We use a similar 3D CNN architecture as in Figure 3with the sizes of kernels and the number of feature maps in each layer modified to consider the 80 \u00d760\u00d79 inputs. In particular, the three convolutional layers use kernels of sizes 9 \u00d77, 7\u00d77, and 6 \u00d74, respectively, and the two subs

finish reason: length
token usageCompletionUsage(completion_tokens=4096, prompt_tokens=9332, total_tokens=13428)
=======================================
sampling layers use kernels of size 3 ×3. By using this setting, the 80 ×60×9 inputs are converted into 128D feature vectors. The final layer consists of 6 units corresponding to the 6 classes. As in ( Jhuang et al. ,2007), we use the data for 16 randomly selected subjects for training, and the data for the other 9 subjects for testing. The recognition performance averaged across 5 random trials is reported in Table 3along with published results in the literature. The 3D CNN model achieves an overall accuracy of 90.2% as compared with 91.7% achieved by the HMAX model. Note that the HMAX model use handcrafted features computed from raw images with 4-fold higher resolution.",
    "conclusion": "We developed a 3D CNN model for action recognition in this paper. This model construct features from both spatial and temporal dimensions by performing 3D convolutions. The developed deep architecture generates multiple channels of information from adjacent input frames and perform convolution and subsampling separately in each channel. The final feature representation is computed by combining information from all channels. We evaluated the 3D CNN model using the TRECVID and the KTH data sets. Results show that the 3D CNN model outperforms compared methods on the TRECVID data, while it achieves competitive performance on the KTH data, demonstrating its superior performance in real-world environments. In this work, we considered the CNN model for action recognition. There are also other deep architectures, such as the deep belief networks ( Hinton et al. , 2006;Lee et al. ,2009a ), which achieve promising performance on object recognition tasks. It would be interesting to extend such models for action recognition. The developed 3D CNN model was trained using supervised algorithm in this work, and it requires a large number of labeled samples. Prior studies show that the number of labeled samples can be significantly reduced when such model is pre-trained using unsupervised algorithms ( Ranzato et al. ,2007). We will explore the unsupervised training of 3D CNN models in the future."
  }
}
```

finish reason: stop
token usageCompletionUsage(completion_tokens=448, prompt_tokens=13532, total_tokens=13980)
=======================================


FINAL RESPONSE (4545 tokens):
{
  "filename": "3D Convolutional Neural Networks for Human Action Recognition",
  "extracted-section-list": [
    "Introduction",
    "3D Convolutional Neural Networks",
    "Related Work",
    "Experiments",
    "Conclusions and Discussions"
  ],
  "target-section-extraction-result": {
    "introduction": "Recognizing human actions in real-world environment finds applications in a variety of domains including intelligent video surveillance, customer attributes, and shopping behavior analysis. However, accurate recognition of actions is a highly challenging task due to cluttered backgrounds, occlusions, and viewpoint variations, etc. Therefore, most of the existing approaches (Efros et al. ,2003;Sch\u00c3\u00bcldt et al. ,2004;Doll\u00c3\u00a1r et al. , 2005;Laptev & P\u00c3\u00a9rez ,2007;Jhuang et al. ,2007) make certain assumptions (e.g., small scale and viewpoint changes) about the circumstances under which the video was taken. However, such assumptions seldom hold in real-world environment. In addition, most of these approaches follow the conventional paradigm of pattern recognition, which consists of two steps in which the first step computes complex handcrafted features from raw video frames and the second step learns classifiers based on the obtained features. In real-world scenarios, it is rarely known which features are important for the task at hand, since the choice of feature is highly problem-dependent. Especially for human action recognition, different action classes may appear dramatically different in terms of their appearances and motion patterns. Deep learning models ( Fukushima ,1980;LeCun et al. , 1998;Hinton & Salakhutdinov ,2006;Hinton et al. , 2006;Bengio ,2009) are a class of machines that can learn a hierarchy of features by building high-level features from low-level ones, thereby automating the process of feature construction. Such learning machines can be trained using either supervised or unsupervised approaches, and the resulting systems have been shown to yield competitive performance in visual object recognition ( LeCun et al. ,1998;Hinton et al. , 2006;Ranzato et al. ,2007;Lee et al. ,2009a), natural language processing ( Collobert & Weston ,2008), and audio classification ( Lee et al. ,2009b ) tasks. The convolutional neural networks (CNNs) ( LeCun et al. , 1998) are a type of deep models in which trainable filters and local neighborhood pooling operations are applied alternatingly on the raw input images, resulting in a hierarchy of increasingly complex features. It has been shown that, when trained with appropriate regularization ( Ahmed et al. ,2008;Yu et al. ,2008; Mobahi et al. ,2009), CNNs can achieve superior performance on visual object recognition tasks without relying on handcrafted features. In addition, CNNs have been shown to be relatively insensitive to certain variations on the inputs ( LeCun et al. ,2004). As a class of attractive deep models for automated feature construction, CNNs have been primarily applied on 2D images. In this paper, we consider the use of CNNs for human action recognition in videos. A simple approach in this direction is to treat video frames as still images and apply CNNs to recognize actions at the individual frame level. Indeed, this approach has been used to analyze the videos of developing embryos ( Ning et al. ,2005). However, such approach does not consider the motion information encoded in multiple contiguous frames. To effectively incorporate the motion information in video analysis, we propose to perform 3D convolution in the convolutional layers of CNNs so that discriminative features along both spatial and temporal dimensions are captured. We show that by applying multiple distinct convolutional operations at the same location on the input, multiple types of features can be extracted. Based on the proposed 3D convolution, a variety of 3D CNN architectures can be devised to analyze video data. We develop a 3D CNN architecture that generates multiple channels of information from adjacent video frames and performs convolution and subsampling separately in each channel. The final feature representation is obtained by combining information from all channels. An additional advantage of the CNN-based models is that the recognition phase is very efficient due to their feed-forward nature. We evaluated the developed 3D CNN model on the TREC Video Retrieval Evaluation (TRECVID) data1, which consist of surveillance video data recorded in London Gatwick Airport. We constructed a multi-module event detection system, which includes 3D CNN as a module, and participated in three tasks of the TRECVID 2009 Evaluation for Surveillance Event Detection. Our system achieved the best performance on all three participated tasks. To provide independent evaluation of the 3D CNN model, we report its performance on the TRECVID 2008 development set in this paper. We also present results on the KTH data as published performance for this data is available. Our experiments show that the developed 3D CNN model outperforms other baseline methods on the TRECVID data, and it achieves competitive performance on the KTH data without depending on handcrafted features, demonstrating that the 3D CNN model is more effective for real-world environments such as those captured in TRECVID data. The experiments also show that the 3D CNN model significantly outperforms the frame-based 2D CNN for most tasks. We also observe that the performance differences between 3D CNN and other methods tend to be larger when the number of positive training samples is small.",
    "method": "In 2D CNNs, 2D convolution is performed at the convolutional layers to extract features from local neighborhood on feature maps in the previous layer. Then an additive bias is applied and the result is passed through a sigmoid function. Formally, the value of unit at position ( x, y) in the jth feature map in the ith layer, denoted as vxy ij, is given by vxy ij= tanh( bij+∑ mPi−1∑ p=0Qi−1∑ q=0wpq ijmv(x+p)(y+q) (i−1)m) , (1) where tanh( ·) is the hyperbolic tangent function, bij is the bias for this feature map, mindexes over the set of feature maps in the ( i−1)th layer connected to the current feature map, wpq ijkis the value at the position ( p, q) of the kernel connected to the kth feature map, and PiandQiare the height and width of the kernel, respectively. In the subsampling layers, the resolution of the feature maps is reduced by pooling over local neighborhood on the feature maps in the previous layer, thereby increasing invariance to distortions on the inputs. A CNN architecture can be constructed by stacking multiple layers of convolution and subsampling in an alternating fashion. The parameters of CNN, such as the bias bijand the kernel weight wpq ijk, are usually trained using either supervised or unsupervised approaches ( LeCun et al. ,1998; Ranzato et al. ,2007). 2.1. 3D Convolution In 2D CNNs, convolutions are applied on the 2D feature maps to compute features from the spatial dimensions only. When applied to video analysis problems, it is desirable to capture the motion information encoded in multiple contiguous frames. To this end, we propose to perform 3D convolutions in the convolution stages of CNNs to compute features from both spatial and temporal dimensions. The 3D convolution is achieved by convolving a 3D kernel to the cube formed by stacking multiple contiguous frames together. By this construction, the feature maps in the convolution layer is connected to multiple contiguous frames in the previous layer, thereby capturing motion information. Formally, the value at position ( x, y, z ) on the jth feature map in the ith layer is given by vxyz ij=tanh( bij+∑ mPi−1∑ p=0Qi−1∑ q=0Ri−1∑ r=0wpqr ijmv(x+p)(y+q)(z+r) (i−1)m) , (2) where Riis the size of the 3D kernel along the temporal dimension, wpqr ijmis the ( p, q, r)th value of the kernel connected to the mth feature map in the previous layer. A comparison of 2D and 3D convolutions is given in Figure 1. Note that a 3D convolutional kernel can only extract one type of features from the frame cube, since the kernel weights are replicated across the entire cube. A general design principle of CNNs is that the number of feature maps should be increased in late layers by generating multiple types of features from the same set of lower-level feature maps. Similar to the case of 2D convolution, this can be achieved by applying multiple 3D convolutions with distinct kernels to the same location in the previous layer (Figure 2). 2.2. A 3D CNN Architecture Based on the 3D convolution described above, a variety of CNN architectures can be devised. In the following, we describe a 3D CNN architecture that we have developed for human action recognition on the TRECVID data set. In this architecture shown in Figure 3, we consider 7 frames of size 60 ×40 centered on the current frame as inputs to the 3D CNN model. We first apply a set of hardwired kernels to generate multiple channels of information from the input frames. This results in 33 feature maps in the second layer in 5 different channels known as gray, gradient-x, gradient-y, optflow-x, and optflow-y. The gray channel contains the gray pixel values of the 7 input frames. The feature maps in the gradient-x and gradient-y channels are obtained by computing gradients along the horizontal and vertical directions, respectively, on each of the 7 input frames, and the optflow-x and optflow-y channels contain the optical flow fields, along the horizontal and vertical directions, respectively, computed from adjacent input frames. This hardwired layer is used to encode our prior knowledge on features, and this scheme usually leads to better performance as compared to random initialization. We then apply 3D convolutions with a kernel size of 7×7×3 (7×7 in the spatial dimension and 3 in the temporal dimension) on each of the 5 channels separately. To increase the number of feature maps, two sets of different convolutions are applied at each location, resulting in 2 sets of feature maps in the C2 layer each consisting of 23 feature maps. This layer contains 1,480 trainable parameters. In the subsequent subsampling layer S3, we apply 2 ×2 subsampling on each of the feature maps in the C2 layer, which leads to the same number of feature maps with reduced spatial resolution. The number of trainable parameters in this layer is 92. The next convolution layer C4 is obtained by applying 3D convolution with a kernel size of 7×6×3 on each of the 5 channels in the two sets of feature maps separately. To increase the number of feature maps, we apply 3 convolutions with different kernels at each location, leading to 6 distinct sets of feature maps in the C4 layer each containing 13 feature maps. This layer contains 3,810 trainable parameters. The next layer S5 is obtained by applying 3×3 subsampling on each feature maps in the C4 layer, which leads to the same number of feature maps with reduced spatial resolution. The number of trainable parameters in this layer is 156. At this stage, the size of the temporal dimension is already relatively small (3 for gray, gradient-x, gradient-y and 2 for optflow-x and optflow-y), so we perform convolution only in the spatial dimension at this layer. The size of the convolution kernel used is 7 ×4 so that the sizes of the output feature maps are reduced to 1 ×1. The C6 layer consists of 128 feature maps of size 1 ×1, and each of them is connected to all the 78 feature maps in the S5 layer, leading to 289,536 trainable parameters. By the multiple layers of convolution and subsampling, the 7 input frames have been converted into a 128D feature vector capturing the motion information in the input frames. The output layer consists of the same number of units as the number of actions, and each unit is fully connected to each of the 128 units in the C6 layer. In this design we essentially apply a linear classifier on the 128D feature vector for action classification. For an action recognition problem with 3 classes, the number of trainable parameters at the output layer is 384. The total number of trainable parameters in this 3D CNN model is 295,458, and all of them are initialized randomly and trained by online error back-propagation algorithm as described in (LeCun et al. ,1998). We have designed and evaluated other 3D CNN architectures that combine multiple channels of information at different stages, and our results show that this architecture gives the best performance.",
    "result": "We perform experiments on the TRECVID 2008 data and the KTH data ( Sch\u00c3\u00bcldt et al. ,2004) to evaluate the developed 3D CNN model for action recognition. 4.1. Action Recognition on TRECVID Data The TRECVID 2008 development data set consists of 49-hour videos captured at the London Gatwick Airport using 5 different cameras with a resolution of 720\u00d7576 at 25 fps. The videos recorded by camera number 4 are excluded as few events occurred in this scene. In this experiments, we focus on the recognition of 3 action classes ( CellToEar ,ObjectPut , andPoint- ing). Each action is classified in the one-against-rest manner, and a large number of negative samples were generated from actions that are not in these 3 classes. This data set was captured on five days (20071101, 20071106, 20071107, 20071108, and 20071112), and the statistics of the data used in our experiments are summarized in Table 1. The 3D CNN model used in this experiment is as described in Section 2and Figure 3, and the number of training iterations are tuned on a separate validation set. As the videos were recorded in real-world environments, and each frame contains multiple humans, we apply a human detector and a detection-driven tracker to locate human heads. Some sample human detection and tracking results are shown in Figure 4. Based on the detection and tracking results, a bounding box for each human that performs action was computed. The multiple frames required by 3D CNN model are obtained by extracting bounding boxes at the same position from consecutive frames before and after the current frame, leading to a cube containing the action. The temporal dimension of the cube is set to 7 in our experiments as it has been shown that 5-7 frames are enough to achieve a performance similar to the one obtainable with the entire video sequence (Schindler & Van Gool ,2008). The frames were extracted with a step size of 2. That is, suppose the current frame is numbered 0, we extract a bounding box at the same position from frames numbered -6, -4, -2, 0, 2, 4, and 6. The patch inside the bounding box on each frame is scaled to 60 \u00d740 pixels. To evaluate the effectiveness of the 3D CNN model, we report the results of the frame-based 2D CNN model. In addition, we compare the 3D CNN model with two other baseline methods, which follow the state-of-the-art bag-of-words (BoW) paradigm in which complex handcrafted features are computed. For each image cube as used in 3D CNN, we construct a BoW feature based on dense local invariant features. Then a one-against-all linear SVM is learned for each action class. Specifically, we extract dense SIFT descriptors ( Lowe, 2004) from raw gray images or motion edge history images (MEHI) ( Yang et al. ,2009). Local features on raw gray images preserve the appearance information, while MEHI concerns with the shape and motion patterns. These SIFT descriptors are calculated every 6 pixels from 7 \u00d77 and 16 \u00d716 local image patches in the same cubes as in the 3D CNN model. Then they are softly quantized using a 512-word codebook to build the BoW features. To exploit the spatial layout information, we employ similar approach as the spatial pyramid matching (SPM) ( Lazebnik et al. ,2006) to partition the candidate region into 2 \u00d72 and 3 \u00d74 cells and concatenate their BoW features. The dimensionality of the entire feature vector is 512 \u00d7(2\u00d72+3\u00d74) = 8192. We denote the method based on gray images as SPMcube grayand the one based on MEHI as SPMcube MEHI. We report the 5-fold cross-validation results in which the data for a single day are used as a fold. The performance measures we used are precision, recall, and area under the ROC curve (ACU) at multiple values of false positive rates (FPR). The performance of the four methods is summarized in Table 2. We can observe from Table 2that the 3D CNN model outperforms the frame-based 2D CNN model, SPMcube gray, and SPMcube MEHI significantly on the action classes CellToEar andOb- jectPut in all cases. For the action class Pointing , 3D CNN model achieves slightly worse performance than the other three methods. From Table 1we can see that the number of positive samples in the Pointing class is significantly larger than those of the other two classes. Hence, we can conclude that the 3D CNN model is more effective when the number of positive samples is small. Overall, the 3D CNN model outperforms other three methods consistently as can be seen from the average performance in Table 2. 4.2. Action Recognition on KTH Data We evaluate the 3D CNN model on the KTH data (Sch\u00c3\u00bcldt et al. ,2004), which consist of 6 action classes performed by 25 subjects. To follow the setup in the HMAX model, we use a 9-frame cube as input and extract foreground as in ( Jhuang et al. ,2007). To reduce the memory requirement, the resolutions of the input frames are reduced to 80 \u00d760 in our experiments as compared to 160 \u00d7120 used in ( Jhuang et al. ,2007). We use a similar 3D CNN architecture as in Figure 3with the sizes of kernels and the number of feature maps in each layer modified to consider the 80 \u00d760\u00d79 inputs. In particular, the three convolutional layers use kernels of sizes 9 \u00d77, 7\u00d77, and 6 \u00d74, respectively, and the two subssampling layers use kernels of size 3 ×3. By using this setting, the 80 ×60×9 inputs are converted into 128D feature vectors. The final layer consists of 6 units corresponding to the 6 classes. As in ( Jhuang et al. ,2007), we use the data for 16 randomly selected subjects for training, and the data for the other 9 subjects for testing. The recognition performance averaged across 5 random trials is reported in Table 3along with published results in the literature. The 3D CNN model achieves an overall accuracy of 90.2% as compared with 91.7% achieved by the HMAX model. Note that the HMAX model use handcrafted features computed from raw images with 4-fold higher resolution.",
    "conclusion": "We developed a 3D CNN model for action recognition in this paper. This model construct features from both spatial and temporal dimensions by performing 3D convolutions. The developed deep architecture generates multiple channels of information from adjacent input frames and perform convolution and subsampling separately in each channel. The final feature representation is computed by combining information from all channels. We evaluated the 3D CNN model using the TRECVID and the KTH data sets. Results show that the 3D CNN model outperforms compared methods on the TRECVID data, while it achieves competitive performance on the KTH data, demonstrating its superior performance in real-world environments. In this work, we considered the CNN model for action recognition. There are also other deep architectures, such as the deep belief networks ( Hinton et al. , 2006;Lee et al. ,2009a ), which achieve promising performance on object recognition tasks. It would be interesting to extend such models for action recognition. The developed 3D CNN model was trained using supervised algorithm in this work, and it requires a large number of labeled samples. Prior studies show that the number of labeled samples can be significantly reduced when such model is pre-trained using unsupervised algorithms ( Ranzato et al. ,2007). We will explore the unsupervised training of 3D CNN models in the future."
  }
}

Total input tokens:22864
Total output tokens:4544
Is valid JSON:True