```json
{
  "filename": "3D Convolutional Neural Networks for Human Action Recognition",
  "extracted-section-list": [
    "Introduction",
    "3D Convolutional Neural Networks",
    "Related Work",
    "Experiments",
    "Conclusions and Discussions"
  ],
  "target-section-extraction-result": {
    "introduction": "Recognizing human actions in real-world environment\n\u00ads applications in a variety of domains including in-\ntelligent video surveillance, customer attributes, and\nshopping behavior analysis. However, accurate recog-\nnition of actions is a highly challenging task due to\nAppearing in Proceedings of the 27thInternational Confer-\nence on Machine Learning , Haifa, Israel, 2010. Copyright\n2010 by the author(s)/owner(s).cluttered backgrounds, occlusions, and viewpoint vari-\nations, etc. Therefore, most of the existing approaches\n(Efros et al. ,2003;Sch\u0308 uldt et al. ,2004;Doll\u0301 ar et al. ,\n2005;Laptev & P\u0301 erez ,2007;Jhuang et al. ,2007)\nmake certain assumptions (e.g., small scale and view-\npoint changes) about the circumstances under which\nthe video was taken. However, such assumptions sel-\ndom hold in real-world environment. In addition, most\nof these approaches follow the conventional paradigm\nof pattern recognition, which consists of two steps in\nwhich the \u00adrst step computes complex handcrafted fea-\ntures from raw video frames and the second step learns\nclassi\u00aders based on the obtained features. In real-world\nscenarios, it is rarely known which features are impor-\ntant for the task at hand, since the choice of feature is\nhighly problem-dependent. Especially for human ac-\ntion recognition, di\u00e8erent action classes may appear\ndramatically di\u00e8erent in terms of their appearances\nand motion patterns.\nDeep learning models ( Fukushima ,1980;LeCun et al. ,\n1998;Hinton & Salakhutdinov ,2006;Hinton et al. ,\n2006;Bengio ,2009) are a class of machines that can\nlearn a hierarchy of features by building high-level\nfeatures from low-level ones, thereby automating the\nprocess of feature construction. Such learning ma-\nchines can be trained using either supervised or un-\nsupervised approaches, and the resulting systems have\nbeen shown to yield competitive performance in visual\nobject recognition ( LeCun et al. ,1998;Hinton et al. ,\n2006;Ranzato et al. ,2007;Lee et al. ,2009a), natu-\nral language processing ( Collobert & Weston ,2008),\nand audio classi\u00acation ( Lee et al. ,2009b ) tasks. The\nconvolutional neural networks (CNNs) ( LeCun et al. ,\n1998) are a type of deep models in which trainable\n\u00adlters and local neighborhood pooling operations are\napplied alternatingly on the raw input images, result-\ning in a hierarchy of increasingly complex features.\nIt has been shown that, when trained with appropri-3D Convolutional Neural Networks for Human Action Recognit ion\nate regularization ( Ahmed et al. ,2008;Yu et al. ,2008;\nMobahi et al. ,2009), CNNs can achieve superior per-\nformance on visual object recognition tasks without\nrelying on handcrafted features. In addition, CNNs\nhave been shown to be relatively insensitive to certain\nvariations on the inputs ( LeCun et al. ,2004).\nAs a class of attractive deep models for automated fea-\nture construction, CNNs have been primarily applied\non 2D images. In this paper, we consider the use of\nCNNs for human action recognition in videos. A sim-\nple approach in this direction is to treat video frames\nas still images and apply CNNs to recognize actions\nat the individual frame level. Indeed, this approach\nhas been used to analyze the videos of developing\nembryos ( Ning et al. ,2005). However, such approach\ndoes not consider the motion information encoded in\nmultiple contiguous frames. To e\u00a8ectively incorporate\nthe motion information in video analysis, we propose\nto perform 3D convolution in the convolutional layers\nof CNNs so that discriminative features along both\nspatial and temporal dimensions are captured. We\nshow that by applying multiple distinct convolutional\noperations at the same location on the input, multi-\nple types of features can be extracted. Based on the\nproposed 3D convolution, a variety of 3D CNN archi-\ntectures can be devised to analyze video data. We\ndevelop a 3D CNN architecture that generates multi-\nple channels of information from adjacent video frames\nand performs convolution and subsampling separately\nin each channel. The \u00adnal feature representation is\nobtained by combining information from all channels.\nAn additional advantage of the CNN-based models is\nthat the recognition phase is very e\u00b3cient due to their\nfeed-forward nature.\nWe evaluated the developed 3D CNN model on the\nTREC Video Retrieval Evaluation (TRECVID) data1,\nwhich consist of surveillance video data recorded in\nLondon Gatwick Airport. We constructed a multi-\nmodule event detection system, which includes 3D\nCNN as a module, and participated in three tasks of\nthe TRECVID 2009 Evaluation for Surveillance Event\nDetection. Our system achieved the best performance\non all three participated tasks. To provide indepen-\ndent evaluation of the 3D CNN model, we report its\nperformance on the TRECVID 2008 development set\nin this paper. We also present results on the KTH\ndata as published performance for this data is avail-\nable. Our experiments show that the developed 3D\nCNN model outperforms other baseline methods on\nthe TRECVID data, and it achieves competitive per-\nformance on the KTH data without depending on\n1http://www-nlpir.nist.gov/projects/trecvid/handcrafted features, demonstrating that the 3D CNN\nmodel is more e\u00a8ective for real-world environments\nsuch as those captured in TRECVID data. The exper-\niments also show that the 3D CNN model signi\u00acantly\noutperforms the frame-based 2D CNN for most tasks.\nWe also observe that the performance di\u00a8erences be-\ntween 3D CNN and other methods tend to be larger\nwhen the number of positive training samples is small.",
    "method": "In 2D CNNs, 2D convolution is performed at the con-\nvolutional layers to extract features from local neigh-\nborhood on feature maps in the previous layer. Then\nan additive bias is applied and the result is passed\nthrough a sigmoid function. Formally, the value of\nunit at position ( x, y) in the jth feature map in the\nith layer, denoted as vxy\nij, is given by\nvxy\nij= tanh(\nbij+\u2211\nmPi\u22121\u2211\np=0Qi\u22121\u2211\nq=0wpq\nijmv(x+p)(y+q)\n(i\u22121)m)\n,\n(1)\nwhere tanh( \u00b7) is the hyperbolic tangent function, bij\nis the bias for this feature map, mindexes over the\nset of feature maps in the ( i\u22121)th layer connected\nto the current feature map, wpq\nijkis the value at the\nposition ( p, q) of the kernel connected to the kth fea-\nture map, and PiandQiare the height and width\nof the kernel, respectively. In the subsampling lay-\ners, the resolution of the feature maps is reduced by\npooling over local neighborhood on the feature maps\nin the previous layer, thereby increasing invariance to\ndistortions on the inputs. A CNN architecture can be\nconstructed by stacking multiple layers of convolution\nand subsampling in an alternating fashion. The pa-\nrameters of CNN, such as the bias bijand the kernel\nweight wpq\nijk, are usually trained using either super-\nvised or unsupervised approaches ( LeCun et al. ,1998;\nRanzato et al. ,2007).\n2.1. 3D Convolution\nIn 2D CNNs, convolutions are applied on the 2D fea-\nture maps to compute features from the spatial dimen-\nsions only. When applied to video analysis problems,\nit is desirable to capture the motion information en-\ncoded in multiple contiguous frames. To this end, we\npropose to perform 3D convolutions in the convolution\nstages of CNNs to compute features from both spa-\ntial and temporal dimensions. The 3D convolution is\nachieved by convolving a 3D kernel to the cube formed\nby stacking multiple contiguous frames together. By\nthis construction, the feature maps in the convolution\nlayer is connected to multiple contiguous frames in the3D Convolutional Neural Networks for Human Action Recognit ion\n(a) 2D convolution\nt\ne\nm\np\no\nr\na\nl\n(b) 3D convolution\nFigure 1. Comparison of 2D (a) and 3D (b) convolutions.\nIn (b) the size of the convolution kernel in the temporal\ndimension is 3, and the sets of connections are color-coded\nso that the shared weights are in the same color. In 3D\nconvolution, the same 3D kernel is applied to overlapping\n3D cubes in the input video to extract motion features.\nprevious layer, thereby capturing motion information.\nFormally, the value at position ( x, y, z ) on the jth fea-\nture map in the ith layer is given by\nvxyz\nij=tanh(\nbij+\u2211\nmPi\u22121\u2211\np=0Qi\u22121\u2211\nq=0Ri\u22121\u2211\nr=0wpqr\nijmv(x+p)(y+q)(z+r)\n(i\u22121)m)\n,\n(2)\nwhere Riis the size of the 3D kernel along the tem-\nporal dimension, wpqr\nijmis the ( p, q, r)th value of the\nkernel connected to the mth feature map in the previ-\nous layer. A comparison of 2D and 3D convolutions is\ngiven in Figure 1.\nNote that a 3D convolutional kernel can only extract\none type of features from the frame cube, since the\nkernel weights are replicated across the entire cube. A\ngeneral design principle of CNNs is that the number\nof feature maps should be increased in late layers by\ngenerating multiple types of features from the samet\ne\nm\np\no\nr\na\nl\nFigure 2. Extraction of multiple features from contiguous\nframes. Multiple 3D convolutions can be applied to con-\ntiguous frames to extract multiple features. As in Figure 1,\nthe sets of connections are color-coded so that the shared\nweights are in the same color. Note that all the 6 sets of\nconnections do not share weights, resulting in two di\u00a8erent\nfeature maps on the right.\nset of lower-level feature maps. Similar to the case\nof 2D convolution, this can be achieved by applying\nmultiple 3D convolutions with distinct kernels to the\nsame location in the previous layer (Figure 2).\n2.2. A 3D CNN Architecture\nBased on the 3D convolution described above, a variety\nof CNN architectures can be devised. In the following,\nwe describe a 3D CNN architecture that we have devel-\noped for human action recognition on the TRECVID\ndata set. In this architecture shown in Figure 3, we\nconsider 7 frames of size 60 \u00d740 centered on the current\nframe as inputs to the 3D CNN model. We \u00adrst apply a\nset of hardwired kernels to generate multiple channels\nof information from the input frames. This results in\n33 feature maps in the second layer in 5 di\u00a8erent chan-\nnels known as gray, gradient-x, gradient-y, opt\u00acow-x,\nand opt\u00acow-y. The gray channel contains the gray\npixel values of the 7 input frames. The feature maps\nin the gradient-x and gradient-y channels are obtained\nby computing gradients along the horizontal and ver-\ntical directions, respectively, on each of the 7 input\nframes, and the opt\u00acow-x and opt\u00acow-y channels con-\ntain the optical \u00acow \u00aelds, along the horizontal and\nvertical directions, respectively, computed from adja-\ncent input frames. This hardwired layer is used to en-\ncode our prior knowledge on features, and this scheme\nusually leads to better performance as compared to\nrandom initialization.3D Convolutional Neural Networks for Human Action Recognit ion\nH1:\n33@60x40\nC2:\n23*2@54x34\n7x7x3 3D\nconvolution\n2x2\nsubsampling\nS3:\n23*2@27x17\n7x6x3 3D\nconvolution\nC4:\n13*6@21x12\n3x3\nsubsampling\nS5:\n13*6@7x4\n7x4\nconvolution\nC6:\n128@1x1\nfull\nconnnection\nhardwired\ninput:\n7@60x40\nFigure 3. A 3D CNN architecture for human action recognition. This arc hitecture consists of 1 hardwired layer, 3 convo-\nlution layers, 2 subsampling layers, and 1 full connection l ayer. Detailed descriptions are given in the text.\nWe then apply 3D convolutions with a kernel size of\n7\u00d77\u00d73 (7\u00d77 in the spatial dimension and 3 in the\ntemporal dimension) on each of the 5 channels sepa-\nrately. To increase the number of feature maps, two\nsets of di\u00a8erent convolutions are applied at each loca-\ntion, resulting in 2 sets of feature maps in the C2 layer\neach consisting of 23 feature maps. This layer con-\ntains 1,480 trainable parameters. In the subsequent\nsubsampling layer S3, we apply 2 \u00d72 subsampling on\neach of the feature maps in the C2 layer, which leads\nto the same number of feature maps with reduced spa-\ntial resolution. The number of trainable parameters in\nthis layer is 92. The next convolution layer C4 is ob-\ntained by applying 3D convolution with a kernel size\nof 7\u00d76\u00d73 on each of the 5 channels in the two sets\nof feature maps separately. To increase the number\nof feature maps, we apply 3 convolutions with di\u00a8er-\nent kernels at each location, leading to 6 distinct sets\nof feature maps in the C4 layer each containing 13\nfeature maps. This layer contains 3,810 trainable pa-\nrameters. The next layer S5 is obtained by applying\n3\u00d73 subsampling on each feature maps in the C4 layer,\nwhich leads to the same number of feature maps with\nreduced spatial resolution. The number of trainable\nparameters in this layer is 156. At this stage, the size\nof the temporal dimension is already relatively small\n(3 for gray, gradient-x, gradient-y and 2 for opt\u00acow-x\nand opt\u00acow-y), so we perform convolution only in the\nspatial dimension at this layer. The size of the con-\nvolution kernel used is 7 \u00d74 so that the sizes of the\noutput feature maps are reduced to 1 \u00d71. The C6 layer\nconsists of 128 feature maps of size 1 \u00d71, and each of\nthem is connected to all the 78 feature maps in the S5\nlayer, leading to 289,536 trainable parameters.\nBy the multiple layers of convolution and subsampling,the 7 input frames have been converted into a 128D\nfeature vector capturing the motion information in the\ninput frames. The output layer consists of the same\nnumber of units as the number of actions, and each\nunit is fully connected to each of the 128 units in\nthe C6 layer. In this design we essentially apply a\nlinear classi\u00ader on the 128D feature vector for action\nclassi\u00acation. For an action recognition problem with\n3 classes, the number of trainable parameters at the\noutput layer is 384. The total number of trainable\nparameters in this 3D CNN model is 295,458, and all\nof them are initialized randomly and trained by on-\nline error back-propagation algorithm as described in\n(LeCun et al. ,1998). We have designed and evalu-\nated other 3D CNN architectures that combine mul-\ntiple channels of information at di\u00a8erent stages, and\nour results show that this architecture gives the best\nperformance.",
    "result": "We perform experiments on the TRECVID 2008 data\nand the KTH data ( Sch\u0308 uldt et al. ,2004) to evaluate\nthe developed 3D CNN model for action recognition.\n4.1. Action Recognition on TRECVID Data\nThe TRECVID 2008 development data set consists of\n49-hour videos captured at the London Gatwick Air-\nport using 5 di\u00a8erent cameras with a resolution of\n720\u00d7576 at 25 fps. The videos recorded by camera\nnumber 4 are excluded as few events occurred in this\nscene. In this experiments, we focus on the recognitionof 3 action classes ( CellToEar ,ObjectPut , andPoint-\ning). Each action is classi\u00aded in the one-against-rest\nmanner, and a large number of negative samples were\ngenerated from actions that are not in these 3 classes.\nThis data set was captured on \u00adve days (20071101,\n20071106,

finish reason: length
token usageCompletionUsage(completion_tokens=4096, prompt_tokens=9343, total_tokens=13439)
=======================================
20071107, 20071108, and 20071112), and the statistics of the data used in our experiments are summarized in Table 1. The 3D CNN model used in this experiment is as described in Section 2and Figure 3, and the number of training iterations are tuned on a separate validation set. As the videos were recorded in real-world environments, and each frame contains multiple humans, we apply a human detector and a detection-driven tracker to locate human heads. Some sample human detection and tracking results are shown in Figure 4. Based on the detection and tracking results, a bounding box for each human that performs action was computed. The multiple frames required by 3D CNN model are obtained by extracting bounding boxes at the same position from consecutive frames before and after the current frame, leading to a cube containing the action. The temporal dimension of the cube is set to 7 in our experiments as it has been shown that 5-7 frames are enough to achieve a performance similar to the one obtainable with the entire video sequence (Schindler & Van Gool ,2008). The frames were extracted with a step size of 2. That is, suppose the current frame is numbered 0, we extract a bounding box at the same position from frames numbered -6, -4, -2, 0, 2, 4, and 6. The patch inside the bounding box on each frame is scaled to 60 ×40 pixels. To evaluate the effectiveness of the 3D CNN model, we report the results of the frame-based 2D CNN model. In addition, we compare the 3D CNN model with two other baseline methods, which follow the state-of-the-art bag-of-words (BoW) paradigm in which complex handcrafted features are computed. For each image cube as used in 3D CNN, we construct a BoW feature based on dense local invariant features. Then a one-against-all linear SVM is learned for each action class. Specifically, we extract dense SIFT descriptors ( Lowe, 2004) from raw gray images or motion edge history images (MEHI) ( Yang et al. ,2009). Local features on raw gray images preserve the appearance information, while MEHI concerns with the shape and motion patterns. These SIFT descriptors are calculated every 6 pixels from 7 ×7 and 16 ×16 local image patches in the same cubes as in the 3D CNN model. Then they are softly quantized using a 512-word codebook to build the BoW features. To exploit the spatial layout information, we employ similar approach as the spatial pyramid matching (SPM) ( Lazebnik et al. ,2006) to partition the candidate region into 2 ×2 and 3 ×4 cells and concatenate their BoW features. The dimensionality of the entire feature vector is 512 ×(2×2+3×4) = 8192. We denote the method based on gray images as SPMcube gray and the one based on MEHI as SPMcube MEHI. We report the 5-fold cross-validation results in which the data for a single day are used as a fold. The performance measures we used are precision, recall, and area under the ROC curve (ACU) at multiple values of false positive rates (FPR). The performance of the four methods is summarized in Table 2. We can observe from Table 2 that the 3D CNN model outperforms the frame-based 2D CNN model, SPMcube gray, and SPMcube MEHI significantly on the action classes CellToEar and ObjectPut in all cases. For the action class Pointing, 3D CNN model achieves slightly worse performance than the other three methods. From Table 1 we can see that the number of positive samples in the Pointing class is significantly larger than those of the other two classes. Hence, we can conclude that the 3D CNN model is more effective when the number of positive samples is small. Overall, the 3D CNN model outperforms other three methods consistently as can be seen from the average performance in Table 2.

4.2. Action Recognition on KTH Data We evaluate the 3D CNN model on the KTH data (Sch¨ uldt et al. ,2004), which consist of 6 action classes performed by 25 subjects. To follow the setup in the HMAX model, we use a 9-frame cube as input and extract foreground as in ( Jhuang et al. ,2007). To reduce the memory requirement, the resolutions of the input frames are reduced to 80 ×60 in our experiments as compared to 160 ×120 used in (Jhuang et al. ,2007). We use a similar 3D CNN architecture as in Figure 3 with the sizes of kernels and the number of feature maps in each layer modified to consider the 80 ×60×9 inputs. In particular, the three convolutional layers use kernels of sizes 9 ×7, 7×7, and 6 ×4, respectively, and the two subsampling layers use kernels of size 3 ×3. By using this setting, the 80 ×60×9 inputs are converted into 128D feature vectors. The final layer consists of 6 units corresponding to the 6 classes. As in ( Jhuang et al. ,2007), we use the data for 16 randomly selected subjects for training, and the data for the other 9 subjects for testing. The recognition performance averaged across 5 random trials is reported in Table 3 along with published results in the literature. The 3D CNN model achieves an overall accuracy of 90.2% as compared with 91.7% achieved by the HMAX model. Note that the HMAX model use handcrafted features computed from raw images with 4-fold higher resolution.",

    "conclusion": "We developed a 3D CNN model for action recognition in this paper. This model construct features from both spatial and temporal dimensions by performing 3D convolutions. The developed deep architecture generates multiple channels of information from adjacent input frames and perform convolution and subsampling separately in each channel. The final feature representation is computed by combining information from all channels. We evaluated the 3D CNN model using the TRECVID and the KTH data sets. Results show that the 3D CNN model outperforms compared methods on the TRECVID data, while it achieves competitive performance on the KTH data, demonstrating its superior performance in real-world environments. In this work, we considered the CNN model for action recognition. There are also other deep architectures, such as the deep belief networks ( Hinton et al. ,2006;Lee et al. ,2009a ), which achieve promising performance on object recognition tasks. It would be interesting to extend such models for action recognition. The developed 3D CNN model was trained using supervised algorithm in this work, and it requires a large number of labeled samples. Prior studies show that the number of labeled samples can be significantly reduced when such model is pre-trained using unsupervised algorithms ( Ranzato et al. ,2007). We will explore the unsupervised training of 3D CNN models in the future. Acknowledgments The main part of this work was done during the internship of the first author at NEC Laboratories America, Inc., Cupertino, CA."
  }
}
```

finish reason: stop
token usageCompletionUsage(completion_tokens=1530, prompt_tokens=13554, total_tokens=15084)
=======================================


FINAL RESPONSE (5626 tokens):
{
  "filename": "3D Convolutional Neural Networks for Human Action Recognition",
  "extracted-section-list": [
    "Introduction",
    "3D Convolutional Neural Networks",
    "Related Work",
    "Experiments",
    "Conclusions and Discussions"
  ],
  "target-section-extraction-result": {
    "introduction": "Recognizing human actions in real-world environment\n\u00ads applications in a variety of domains including in-\ntelligent video surveillance, customer attributes, and\nshopping behavior analysis. However, accurate recog-\nnition of actions is a highly challenging task due to\nAppearing in Proceedings of the 27thInternational Confer-\nence on Machine Learning , Haifa, Israel, 2010. Copyright\n2010 by the author(s)/owner(s).cluttered backgrounds, occlusions, and viewpoint vari-\nations, etc. Therefore, most of the existing approaches\n(Efros et al. ,2003;Sch\u0308 uldt et al. ,2004;Doll\u0301 ar et al. ,\n2005;Laptev & P\u0301 erez ,2007;Jhuang et al. ,2007)\nmake certain assumptions (e.g., small scale and view-\npoint changes) about the circumstances under which\nthe video was taken. However, such assumptions sel-\ndom hold in real-world environment. In addition, most\nof these approaches follow the conventional paradigm\nof pattern recognition, which consists of two steps in\nwhich the \u00adrst step computes complex handcrafted fea-\ntures from raw video frames and the second step learns\nclassi\u00aders based on the obtained features. In real-world\nscenarios, it is rarely known which features are impor-\ntant for the task at hand, since the choice of feature is\nhighly problem-dependent. Especially for human ac-\ntion recognition, di\u00e8erent action classes may appear\ndramatically di\u00e8erent in terms of their appearances\nand motion patterns.\nDeep learning models ( Fukushima ,1980;LeCun et al. ,\n1998;Hinton & Salakhutdinov ,2006;Hinton et al. ,\n2006;Bengio ,2009) are a class of machines that can\nlearn a hierarchy of features by building high-level\nfeatures from low-level ones, thereby automating the\nprocess of feature construction. Such learning ma-\nchines can be trained using either supervised or un-\nsupervised approaches, and the resulting systems have\nbeen shown to yield competitive performance in visual\nobject recognition ( LeCun et al. ,1998;Hinton et al. ,\n2006;Ranzato et al. ,2007;Lee et al. ,2009a), natu-\nral language processing ( Collobert & Weston ,2008),\nand audio classi\u00acation ( Lee et al. ,2009b ) tasks. The\nconvolutional neural networks (CNNs) ( LeCun et al. ,\n1998) are a type of deep models in which trainable\n\u00adlters and local neighborhood pooling operations are\napplied alternatingly on the raw input images, result-\ning in a hierarchy of increasingly complex features.\nIt has been shown that, when trained with appropri-3D Convolutional Neural Networks for Human Action Recognit ion\nate regularization ( Ahmed et al. ,2008;Yu et al. ,2008;\nMobahi et al. ,2009), CNNs can achieve superior per-\nformance on visual object recognition tasks without\nrelying on handcrafted features. In addition, CNNs\nhave been shown to be relatively insensitive to certain\nvariations on the inputs ( LeCun et al. ,2004).\nAs a class of attractive deep models for automated fea-\nture construction, CNNs have been primarily applied\non 2D images. In this paper, we consider the use of\nCNNs for human action recognition in videos. A sim-\nple approach in this direction is to treat video frames\nas still images and apply CNNs to recognize actions\nat the individual frame level. Indeed, this approach\nhas been used to analyze the videos of developing\nembryos ( Ning et al. ,2005). However, such approach\ndoes not consider the motion information encoded in\nmultiple contiguous frames. To e\u00a8ectively incorporate\nthe motion information in video analysis, we propose\nto perform 3D convolution in the convolutional layers\nof CNNs so that discriminative features along both\nspatial and temporal dimensions are captured. We\nshow that by applying multiple distinct convolutional\noperations at the same location on the input, multi-\nple types of features can be extracted. Based on the\nproposed 3D convolution, a variety of 3D CNN archi-\ntectures can be devised to analyze video data. We\ndevelop a 3D CNN architecture that generates multi-\nple channels of information from adjacent video frames\nand performs convolution and subsampling separately\nin each channel. The \u00adnal feature representation is\nobtained by combining information from all channels.\nAn additional advantage of the CNN-based models is\nthat the recognition phase is very e\u00b3cient due to their\nfeed-forward nature.\nWe evaluated the developed 3D CNN model on the\nTREC Video Retrieval Evaluation (TRECVID) data1,\nwhich consist of surveillance video data recorded in\nLondon Gatwick Airport. We constructed a multi-\nmodule event detection system, which includes 3D\nCNN as a module, and participated in three tasks of\nthe TRECVID 2009 Evaluation for Surveillance Event\nDetection. Our system achieved the best performance\non all three participated tasks. To provide indepen-\ndent evaluation of the 3D CNN model, we report its\nperformance on the TRECVID 2008 development set\nin this paper. We also present results on the KTH\ndata as published performance for this data is avail-\nable. Our experiments show that the developed 3D\nCNN model outperforms other baseline methods on\nthe TRECVID data, and it achieves competitive per-\nformance on the KTH data without depending on\n1http://www-nlpir.nist.gov/projects/trecvid/handcrafted features, demonstrating that the 3D CNN\nmodel is more e\u00a8ective for real-world environments\nsuch as those captured in TRECVID data. The exper-\niments also show that the 3D CNN model signi\u00acantly\noutperforms the frame-based 2D CNN for most tasks.\nWe also observe that the performance di\u00a8erences be-\ntween 3D CNN and other methods tend to be larger\nwhen the number of positive training samples is small.",
    "method": "In 2D CNNs, 2D convolution is performed at the con-\nvolutional layers to extract features from local neigh-\nborhood on feature maps in the previous layer. Then\nan additive bias is applied and the result is passed\nthrough a sigmoid function. Formally, the value of\nunit at position ( x, y) in the jth feature map in the\nith layer, denoted as vxy\nij, is given by\nvxy\nij= tanh(\nbij+\u2211\nmPi\u22121\u2211\np=0Qi\u22121\u2211\nq=0wpq\nijmv(x+p)(y+q)\n(i\u22121)m)\n,\n(1)\nwhere tanh( \u00b7) is the hyperbolic tangent function, bij\nis the bias for this feature map, mindexes over the\nset of feature maps in the ( i\u22121)th layer connected\nto the current feature map, wpq\nijkis the value at the\nposition ( p, q) of the kernel connected to the kth fea-\nture map, and PiandQiare the height and width\nof the kernel, respectively. In the subsampling lay-\ners, the resolution of the feature maps is reduced by\npooling over local neighborhood on the feature maps\nin the previous layer, thereby increasing invariance to\ndistortions on the inputs. A CNN architecture can be\nconstructed by stacking multiple layers of convolution\nand subsampling in an alternating fashion. The pa-\nrameters of CNN, such as the bias bijand the kernel\nweight wpq\nijk, are usually trained using either super-\nvised or unsupervised approaches ( LeCun et al. ,1998;\nRanzato et al. ,2007).\n2.1. 3D Convolution\nIn 2D CNNs, convolutions are applied on the 2D fea-\nture maps to compute features from the spatial dimen-\nsions only. When applied to video analysis problems,\nit is desirable to capture the motion information en-\ncoded in multiple contiguous frames. To this end, we\npropose to perform 3D convolutions in the convolution\nstages of CNNs to compute features from both spa-\ntial and temporal dimensions. The 3D convolution is\nachieved by convolving a 3D kernel to the cube formed\nby stacking multiple contiguous frames together. By\nthis construction, the feature maps in the convolution\nlayer is connected to multiple contiguous frames in the3D Convolutional Neural Networks for Human Action Recognit ion\n(a) 2D convolution\nt\ne\nm\np\no\nr\na\nl\n(b) 3D convolution\nFigure 1. Comparison of 2D (a) and 3D (b) convolutions.\nIn (b) the size of the convolution kernel in the temporal\ndimension is 3, and the sets of connections are color-coded\nso that the shared weights are in the same color. In 3D\nconvolution, the same 3D kernel is applied to overlapping\n3D cubes in the input video to extract motion features.\nprevious layer, thereby capturing motion information.\nFormally, the value at position ( x, y, z ) on the jth fea-\nture map in the ith layer is given by\nvxyz\nij=tanh(\nbij+\u2211\nmPi\u22121\u2211\np=0Qi\u22121\u2211\nq=0Ri\u22121\u2211\nr=0wpqr\nijmv(x+p)(y+q)(z+r)\n(i\u22121)m)\n,\n(2)\nwhere Riis the size of the 3D kernel along the tem-\nporal dimension, wpqr\nijmis the ( p, q, r)th value of the\nkernel connected to the mth feature map in the previ-\nous layer. A comparison of 2D and 3D convolutions is\ngiven in Figure 1.\nNote that a 3D convolutional kernel can only extract\none type of features from the frame cube, since the\nkernel weights are replicated across the entire cube. A\ngeneral design principle of CNNs is that the number\nof feature maps should be increased in late layers by\ngenerating multiple types of features from the samet\ne\nm\np\no\nr\na\nl\nFigure 2. Extraction of multiple features from contiguous\nframes. Multiple 3D convolutions can be applied to con-\ntiguous frames to extract multiple features. As in Figure 1,\nthe sets of connections are color-coded so that the shared\nweights are in the same color. Note that all the 6 sets of\nconnections do not share weights, resulting in two di\u00a8erent\nfeature maps on the right.\nset of lower-level feature maps. Similar to the case\nof 2D convolution, this can be achieved by applying\nmultiple 3D convolutions with distinct kernels to the\nsame location in the previous layer (Figure 2).\n2.2. A 3D CNN Architecture\nBased on the 3D convolution described above, a variety\nof CNN architectures can be devised. In the following,\nwe describe a 3D CNN architecture that we have devel-\noped for human action recognition on the TRECVID\ndata set. In this architecture shown in Figure 3, we\nconsider 7 frames of size 60 \u00d740 centered on the current\nframe as inputs to the 3D CNN model. We \u00adrst apply a\nset of hardwired kernels to generate multiple channels\nof information from the input frames. This results in\n33 feature maps in the second layer in 5 di\u00a8erent chan-\nnels known as gray, gradient-x, gradient-y, opt\u00acow-x,\nand opt\u00acow-y. The gray channel contains the gray\npixel values of the 7 input frames. The feature maps\nin the gradient-x and gradient-y channels are obtained\nby computing gradients along the horizontal and ver-\ntical directions, respectively, on each of the 7 input\nframes, and the opt\u00acow-x and opt\u00acow-y channels con-\ntain the optical \u00acow \u00aelds, along the horizontal and\nvertical directions, respectively, computed from adja-\ncent input frames. This hardwired layer is used to en-\ncode our prior knowledge on features, and this scheme\nusually leads to better performance as compared to\nrandom initialization.3D Convolutional Neural Networks for Human Action Recognit ion\nH1:\n33@60x40\nC2:\n23*2@54x34\n7x7x3 3D\nconvolution\n2x2\nsubsampling\nS3:\n23*2@27x17\n7x6x3 3D\nconvolution\nC4:\n13*6@21x12\n3x3\nsubsampling\nS5:\n13*6@7x4\n7x4\nconvolution\nC6:\n128@1x1\nfull\nconnnection\nhardwired\ninput:\n7@60x40\nFigure 3. A 3D CNN architecture for human action recognition. This arc hitecture consists of 1 hardwired layer, 3 convo-\nlution layers, 2 subsampling layers, and 1 full connection l ayer. Detailed descriptions are given in the text.\nWe then apply 3D convolutions with a kernel size of\n7\u00d77\u00d73 (7\u00d77 in the spatial dimension and 3 in the\ntemporal dimension) on each of the 5 channels sepa-\nrately. To increase the number of feature maps, two\nsets of di\u00a8erent convolutions are applied at each loca-\ntion, resulting in 2 sets of feature maps in the C2 layer\neach consisting of 23 feature maps. This layer con-\ntains 1,480 trainable parameters. In the subsequent\nsubsampling layer S3, we apply 2 \u00d72 subsampling on\neach of the feature maps in the C2 layer, which leads\nto the same number of feature maps with reduced spa-\ntial resolution. The number of trainable parameters in\nthis layer is 92. The next convolution layer C4 is ob-\ntained by applying 3D convolution with a kernel size\nof 7\u00d76\u00d73 on each of the 5 channels in the two sets\nof feature maps separately. To increase the number\nof feature maps, we apply 3 convolutions with di\u00a8er-\nent kernels at each location, leading to 6 distinct sets\nof feature maps in the C4 layer each containing 13\nfeature maps. This layer contains 3,810 trainable pa-\nrameters. The next layer S5 is obtained by applying\n3\u00d73 subsampling on each feature maps in the C4 layer,\nwhich leads to the same number of feature maps with\nreduced spatial resolution. The number of trainable\nparameters in this layer is 156. At this stage, the size\nof the temporal dimension is already relatively small\n(3 for gray, gradient-x, gradient-y and 2 for opt\u00acow-x\nand opt\u00acow-y), so we perform convolution only in the\nspatial dimension at this layer. The size of the con-\nvolution kernel used is 7 \u00d74 so that the sizes of the\noutput feature maps are reduced to 1 \u00d71. The C6 layer\nconsists of 128 feature maps of size 1 \u00d71, and each of\nthem is connected to all the 78 feature maps in the S5\nlayer, leading to 289,536 trainable parameters.\nBy the multiple layers of convolution and subsampling,the 7 input frames have been converted into a 128D\nfeature vector capturing the motion information in the\ninput frames. The output layer consists of the same\nnumber of units as the number of actions, and each\nunit is fully connected to each of the 128 units in\nthe C6 layer. In this design we essentially apply a\nlinear classi\u00ader on the 128D feature vector for action\nclassi\u00acation. For an action recognition problem with\n3 classes, the number of trainable parameters at the\noutput layer is 384. The total number of trainable\nparameters in this 3D CNN model is 295,458, and all\nof them are initialized randomly and trained by on-\nline error back-propagation algorithm as described in\n(LeCun et al. ,1998). We have designed and evalu-\nated other 3D CNN architectures that combine mul-\ntiple channels of information at di\u00a8erent stages, and\nour results show that this architecture gives the best\nperformance.",
    "result": "We perform experiments on the TRECVID 2008 data\nand the KTH data ( Sch\u0308 uldt et al. ,2004) to evaluate\nthe developed 3D CNN model for action recognition.\n4.1. Action Recognition on TRECVID Data\nThe TRECVID 2008 development data set consists of\n49-hour videos captured at the London Gatwick Air-\nport using 5 di\u00a8erent cameras with a resolution of\n720\u00d7576 at 25 fps. The videos recorded by camera\nnumber 4 are excluded as few events occurred in this\nscene. In this experiments, we focus on the recognitionof 3 action classes ( CellToEar ,ObjectPut , andPoint-\ning). Each action is classi\u00aded in the one-against-rest\nmanner, and a large number of negative samples were\ngenerated from actions that are not in these 3 classes.\nThis data set was captured on \u00adve days (20071101,\n20071106,20071107, 20071108, and 20071112), and the statistics of the data used in our experiments are summarized in Table 1. The 3D CNN model used in this experiment is as described in Section 2and Figure 3, and the number of training iterations are tuned on a separate validation set. As the videos were recorded in real-world environments, and each frame contains multiple humans, we apply a human detector and a detection-driven tracker to locate human heads. Some sample human detection and tracking results are shown in Figure 4. Based on the detection and tracking results, a bounding box for each human that performs action was computed. The multiple frames required by 3D CNN model are obtained by extracting bounding boxes at the same position from consecutive frames before and after the current frame, leading to a cube containing the action. The temporal dimension of the cube is set to 7 in our experiments as it has been shown that 5-7 frames are enough to achieve a performance similar to the one obtainable with the entire video sequence (Schindler & Van Gool ,2008). The frames were extracted with a step size of 2. That is, suppose the current frame is numbered 0, we extract a bounding box at the same position from frames numbered -6, -4, -2, 0, 2, 4, and 6. The patch inside the bounding box on each frame is scaled to 60 ×40 pixels. To evaluate the effectiveness of the 3D CNN model, we report the results of the frame-based 2D CNN model. In addition, we compare the 3D CNN model with two other baseline methods, which follow the state-of-the-art bag-of-words (BoW) paradigm in which complex handcrafted features are computed. For each image cube as used in 3D CNN, we construct a BoW feature based on dense local invariant features. Then a one-against-all linear SVM is learned for each action class. Specifically, we extract dense SIFT descriptors ( Lowe, 2004) from raw gray images or motion edge history images (MEHI) ( Yang et al. ,2009). Local features on raw gray images preserve the appearance information, while MEHI concerns with the shape and motion patterns. These SIFT descriptors are calculated every 6 pixels from 7 ×7 and 16 ×16 local image patches in the same cubes as in the 3D CNN model. Then they are softly quantized using a 512-word codebook to build the BoW features. To exploit the spatial layout information, we employ similar approach as the spatial pyramid matching (SPM) ( Lazebnik et al. ,2006) to partition the candidate region into 2 ×2 and 3 ×4 cells and concatenate their BoW features. The dimensionality of the entire feature vector is 512 ×(2×2+3×4) = 8192. We denote the method based on gray images as SPMcube gray and the one based on MEHI as SPMcube MEHI. We report the 5-fold cross-validation results in which the data for a single day are used as a fold. The performance measures we used are precision, recall, and area under the ROC curve (ACU) at multiple values of false positive rates (FPR). The performance of the four methods is summarized in Table 2. We can observe from Table 2 that the 3D CNN model outperforms the frame-based 2D CNN model, SPMcube gray, and SPMcube MEHI significantly on the action classes CellToEar and ObjectPut in all cases. For the action class Pointing, 3D CNN model achieves slightly worse performance than the other three methods. From Table 1 we can see that the number of positive samples in the Pointing class is significantly larger than those of the other two classes. Hence, we can conclude that the 3D CNN model is more effective when the number of positive samples is small. Overall, the 3D CNN model outperforms other three methods consistently as can be seen from the average performance in Table 2.

4.2. Action Recognition on KTH Data We evaluate the 3D CNN model on the KTH data (Sch¨ uldt et al. ,2004), which consist of 6 action classes performed by 25 subjects. To follow the setup in the HMAX model, we use a 9-frame cube as input and extract foreground as in ( Jhuang et al. ,2007). To reduce the memory requirement, the resolutions of the input frames are reduced to 80 ×60 in our experiments as compared to 160 ×120 used in (Jhuang et al. ,2007). We use a similar 3D CNN architecture as in Figure 3 with the sizes of kernels and the number of feature maps in each layer modified to consider the 80 ×60×9 inputs. In particular, the three convolutional layers use kernels of sizes 9 ×7, 7×7, and 6 ×4, respectively, and the two subsampling layers use kernels of size 3 ×3. By using this setting, the 80 ×60×9 inputs are converted into 128D feature vectors. The final layer consists of 6 units corresponding to the 6 classes. As in ( Jhuang et al. ,2007), we use the data for 16 randomly selected subjects for training, and the data for the other 9 subjects for testing. The recognition performance averaged across 5 random trials is reported in Table 3 along with published results in the literature. The 3D CNN model achieves an overall accuracy of 90.2% as compared with 91.7% achieved by the HMAX model. Note that the HMAX model use handcrafted features computed from raw images with 4-fold higher resolution.",

    "conclusion": "We developed a 3D CNN model for action recognition in this paper. This model construct features from both spatial and temporal dimensions by performing 3D convolutions. The developed deep architecture generates multiple channels of information from adjacent input frames and perform convolution and subsampling separately in each channel. The final feature representation is computed by combining information from all channels. We evaluated the 3D CNN model using the TRECVID and the KTH data sets. Results show that the 3D CNN model outperforms compared methods on the TRECVID data, while it achieves competitive performance on the KTH data, demonstrating its superior performance in real-world environments. In this work, we considered the CNN model for action recognition. There are also other deep architectures, such as the deep belief networks ( Hinton et al. ,2006;Lee et al. ,2009a ), which achieve promising performance on object recognition tasks. It would be interesting to extend such models for action recognition. The developed 3D CNN model was trained using supervised algorithm in this work, and it requires a large number of labeled samples. Prior studies show that the number of labeled samples can be significantly reduced when such model is pre-trained using unsupervised algorithms ( Ranzato et al. ,2007). We will explore the unsupervised training of 3D CNN models in the future. Acknowledgments The main part of this work was done during the internship of the first author at NEC Laboratories America, Inc., Cupertino, CA."
  }
}

Total input tokens:22897
Total output tokens:5626
Is valid JSON:False