```json
{
  "filename": "The Graph Neural Network Model",
  "extracted-section-list": ["Introduction", "Method", "Result", "Conclusion"],
  "target-section-extraction-result": {
      "introduction": "DATA can be naturally represented by graph structures in several application areas, including proteomics [1], image analysis [2], scene description [3], [4], software engineering [5], [6], and natural language processing [7]. The simplest kinds of graph structures include single nodes and sequences. But in several applications, the information is organized in more complex graph structures such as trees, acyclic graphs, or cyclic graphs. Traditionally, data relationships exploitation has been the subject of many studies in the community of inductive logic programming and, recently, this research theme has been evolving in different directions [8], also because of the applications of relevant concepts in statistics and neural networks to such areas (see, for example, the recent workshops [9]–[12]). In machine learning, structured data is often associated with the goal of (supervised or unsupervised) learning from examples a function tau(G,n) isin IRm that maps a graph G and one of its nodes n to a vector of reals1: tau(G,n) isin IRm. Applications to a graphical domain can generally be divided into two broad classes, called graph-focused and node-focused applications, respectively, in this paper. In graph-focused applications, the function tau is independent of the node n and implements a classifier or a regressor on a graph structured data set. For example, a chemical compound can be modeled by a graph G, the nodes of which stand for atoms (or chemical groups) and the edges of which represent chemical bonds [see Fig. 1(a)] linking together some of the atoms. The mapping tau may be used to estimate the probability that the chemical compound causes a certain disease [13]. In Fig. 1(b), an image is represented by a region adjacency graph where nodes denote homogeneous regions of intensity of the image and arcs represent their adjacency relationship [14]. In this case, tau may be used to classify the image into different classes according to its contents, e.g., castles, cars, people, and so on. In node-focused applications, tau depends on the node n, so that the classification (or the regression) depends on the properties of each node. Object detection is an example of this class of applications. It consists of finding whether an image contains a given object, and, if so, localizing its position [15]. This problem can be solved by a function tau, which classifies the nodes of the region adjacency graph according to whether the corresponding region belongs to the object. For example, the output of tau for Fig. 1(b) might be 1 for black nodes, which correspond to the castle, and 0 otherwise. Another example comes from web page classification. The web can be represented by a graph where nodes stand for pages and edges represent the hyperlinks between them [Fig. 1(c)]. The web connectivity can be exploited, along with page contents, for several purposes, e.g., classifying the pages into a set of topics. Traditional machine learning applications cope with graph structured data by using a preprocessing phase which maps the graph structured information to a simpler representation, e.g., vectors of reals [16]. In other words, the preprocessing step first “squashes” the graph structured data into a vector of reals and then deals with the preprocessed data using a list-based data processing technique. However, important information, e.g., the topological dependency of information on each node may be lost during the preprocessing stage and the final result may depend, in an unpredictable manner, on the details of the preprocessing algorithm. More recently, there have been various approaches [17], [18] attempting to preserve the graph structured nature of the data for as long as required before the processing phase. The idea is to encode the underlying graph structured data using the topological relationships among the nodes of the graph, in order to incorporate graph structured information in the data processing step. Recursive neural networks [17], [19], [20] and Markov chains [18], [21], [22] belong to this set of techniques and are commonly applied both to graph and node-focused problems. The method presented in this paper extends these two approaches in that it can deal directly with graph structured information.",
      "method": "We begin by introducing some notations that will be used throughout the paper. A graph G is a pair (V,E), where V is the set of nodes and E is the set of edges. The set N(v) stands for the neighbors of v, i.e., the nodes connected to v by an arc, while E(v) denotes the set of arcs having v as a vertex. Nodes and edges may have labels represented by real vectors. The labels attached to node v and edge e will be represented by lv and le, respectively. Let L denote the vector obtained by stacking together all the labels of the graph. The notation adopted for labels follows a more general scheme: if x is a vector that contains data from a graph and S is a subset of the nodes (the edges), then x(S) denotes the vector obtained by selecting from x the components related to the node (the edges) in S. For example, lv(N(v)) stands for the vector containing the labels of all the neighbors of v. Labels usually include features of objects related to nodes and features of the relationships between the objects. For example, in the case of an image as in Fig. 1(b), node labels might represent properties of the regions (e.g., area, perimeter, and average color intensity), while edge labels might represent the relative position of the regions (e.g., the distance between their barycenters and the angle between their principal axes). No assumption is made on the arcs; directed and undirected edges are both permitted. However, when different kinds of edges coexist in the same data set, it is necessary to distinguish them. This can be easily achieved by attaching a proper label to each edge. In this case, different kinds of arcs turn out to be just arcs with different labels. The considered graphs may be either positional or nonpositional. Nonpositional graphs are those described so far; positional graphs differ since a unique integer identifier is assigned to each neighbors of a node v to indicate its logical position. Formally, for each node v in a positional graph, there exists an injective function posv : N(v) -> {1,2,...,|N(v)|}, which assigns to each neighbor u of v a position posv(u). Note that the position of the neighbor can be implicitly used for storing useful information. For instance, let us consider the example of the region adjacency graph [see Fig. 1(b)]: posv might enumerate the neighbors of a node v, which represents the adjacent regions, following a clockwise ordering convention. The domain considered in this paper is the set D of pairs of a graph and a node, i.e., D = {(G,v) | G isin G, v isin V(G)}. We assume a supervised learning framework with the learning set L = {((G1,v1),t1),...,((Gm,vm),tm)}, where vi denotes the ith node in the set V(G) and ti is the desired target associated to vi. Finally, G = UGi and V = UV(Gi). Interestingly, all the graphs of the learning set can be combined into a unique disconnected graph, and, therefore, one might think of the learning set as the pair (G,V) where G is a graph and V a is set of pairs (vi,ti). It is worth mentioning that this compact definition is not only useful for its simplicity, but that it also captures directly the very nature of some problems where the domain consists of only one graph, for instance, a large portion of the web [see Fig. 1(c)].",
      "result": "In this section, we present the experimental results, obtained on a set of simple problems carried out to study the properties of the GNN model and to prove that the method can be applied to relevant applications in relational domains. The problems that we consider, viz., the subgraph matching, the mutagenesis, and the web page ranking, have been selected since they are particularly suited to discover the properties of the model and are correlated to important real-world applications. From a practical point of view, we will see that the results obtained on some parts of mutagenesis data sets are among the best that are currently reported in the open literature (please see detailed comparison in Section IV-B). Moreover, the subgraph matching problem is relevant to several application domains. Even if the performance of our method is not comparable in terms of best accuracy on the same problem with the most efficient algorithms in the literature, the proposed approach is a very general technique that can be applied on extension of the subgraph matching problems [73]–[75]. Finally, the web page ranking is an interesting problem, since it is important in information retrieval and very few techniques have been proposed for its solution [76]. It is worth mentioning that the GNN model has been already successfully applied on larger applications, which include image classification and object localization in images [77], [78], web page ranking [79], relational learning [80], and XML classification [81].",
      "conclusion": "In this paper, we introduced a novel neural network model that can handle graph inputs: the graphs can be cyclic, directed, undirected, or a mixture of these. The model is based on information diffusion and relaxation mechanisms. The approach extends into a common framework, the previous connectionist techniques for processing structured data, and the methods based on random walk models. A learning algorithm to estimate model parameters was provided and its computational complexity was studied, demonstrating that the method is suitable also for large data sets. Some promising experimental results were provided to assess the model. In particular, the results achieved on the whole Mutagenisis data set and on the unfriendly part of such a data set are the best compared with those reported in the open literature. Moreover, the experiments on the subgraph matching and on the web page ranking show that the method can be applied to problems that are related to important practical applications. The possibility of dealing with domains where the data consists of patterns and relationships gives rise to several new topics of research. For example, while in this paper it is assumed that the domain is static, it may happen that the input graphs change with time. In this case, at least two interesting issues can be considered: first, GNNs must be extended to cope with a dynamic domain; and second, no method exists, to the best of our knowledge, to model the evolution of the domain. The solution of the latter problem, for instance, may allow to model the evolution of the web and, more generally, of social networks. Another topic of future research is the study on how to deal with domains where the relationships, which are not known in advance, must be inferred. In this case, the input contains flat data and is automatically transformed into a set of graphs in order to shed some light on possible hidden relationships."
  }
}
```

finish reason: stop
token usageCompletionUsage(completion_tokens=2310, prompt_tokens=26385, total_tokens=28695)
=======================================


FINAL RESPONSE (2309 tokens):
{   "filename": "The Graph Neural Network Model",   "extracted-section-list": ["Introduction", "Method", "Result", "Conclusion"],   "target-section-extraction-result": {       "introduction": "DATA can be naturally represented by graph structures in several application areas, including proteomics [1], image analysis [2], scene description [3], [4], software engineering [5], [6], and natural language processing [7]. The simplest kinds of graph structures include single nodes and sequences. But in several applications, the information is organized in more complex graph structures such as trees, acyclic graphs, or cyclic graphs. Traditionally, data relationships exploitation has been the subject of many studies in the community of inductive logic programming and, recently, this research theme has been evolving in different directions [8], also because of the applications of relevant concepts in statistics and neural networks to such areas (see, for example, the recent workshops [9]–[12]). In machine learning, structured data is often associated with the goal of (supervised or unsupervised) learning from examples a function tau(G,n) isin IRm that maps a graph G and one of its nodes n to a vector of reals1: tau(G,n) isin IRm. Applications to a graphical domain can generally be divided into two broad classes, called graph-focused and node-focused applications, respectively, in this paper. In graph-focused applications, the function tau is independent of the node n and implements a classifier or a regressor on a graph structured data set. For example, a chemical compound can be modeled by a graph G, the nodes of which stand for atoms (or chemical groups) and the edges of which represent chemical bonds [see Fig. 1(a)] linking together some of the atoms. The mapping tau may be used to estimate the probability that the chemical compound causes a certain disease [13]. In Fig. 1(b), an image is represented by a region adjacency graph where nodes denote homogeneous regions of intensity of the image and arcs represent their adjacency relationship [14]. In this case, tau may be used to classify the image into different classes according to its contents, e.g., castles, cars, people, and so on. In node-focused applications, tau depends on the node n, so that the classification (or the regression) depends on the properties of each node. Object detection is an example of this class of applications. It consists of finding whether an image contains a given object, and, if so, localizing its position [15]. This problem can be solved by a function tau, which classifies the nodes of the region adjacency graph according to whether the corresponding region belongs to the object. For example, the output of tau for Fig. 1(b) might be 1 for black nodes, which correspond to the castle, and 0 otherwise. Another example comes from web page classification. The web can be represented by a graph where nodes stand for pages and edges represent the hyperlinks between them [Fig. 1(c)]. The web connectivity can be exploited, along with page contents, for several purposes, e.g., classifying the pages into a set of topics. Traditional machine learning applications cope with graph structured data by using a preprocessing phase which maps the graph structured information to a simpler representation, e.g., vectors of reals [16]. In other words, the preprocessing step first “squashes” the graph structured data into a vector of reals and then deals with the preprocessed data using a list-based data processing technique. However, important information, e.g., the topological dependency of information on each node may be lost during the preprocessing stage and the final result may depend, in an unpredictable manner, on the details of the preprocessing algorithm. More recently, there have been various approaches [17], [18] attempting to preserve the graph structured nature of the data for as long as required before the processing phase. The idea is to encode the underlying graph structured data using the topological relationships among the nodes of the graph, in order to incorporate graph structured information in the data processing step. Recursive neural networks [17], [19], [20] and Markov chains [18], [21], [22] belong to this set of techniques and are commonly applied both to graph and node-focused problems. The method presented in this paper extends these two approaches in that it can deal directly with graph structured information.",       "method": "We begin by introducing some notations that will be used throughout the paper. A graph G is a pair (V,E), where V is the set of nodes and E is the set of edges. The set N(v) stands for the neighbors of v, i.e., the nodes connected to v by an arc, while E(v) denotes the set of arcs having v as a vertex. Nodes and edges may have labels represented by real vectors. The labels attached to node v and edge e will be represented by lv and le, respectively. Let L denote the vector obtained by stacking together all the labels of the graph. The notation adopted for labels follows a more general scheme: if x is a vector that contains data from a graph and S is a subset of the nodes (the edges), then x(S) denotes the vector obtained by selecting from x the components related to the node (the edges) in S. For example, lv(N(v)) stands for the vector containing the labels of all the neighbors of v. Labels usually include features of objects related to nodes and features of the relationships between the objects. For example, in the case of an image as in Fig. 1(b), node labels might represent properties of the regions (e.g., area, perimeter, and average color intensity), while edge labels might represent the relative position of the regions (e.g., the distance between their barycenters and the angle between their principal axes). No assumption is made on the arcs; directed and undirected edges are both permitted. However, when different kinds of edges coexist in the same data set, it is necessary to distinguish them. This can be easily achieved by attaching a proper label to each edge. In this case, different kinds of arcs turn out to be just arcs with different labels. The considered graphs may be either positional or nonpositional. Nonpositional graphs are those described so far; positional graphs differ since a unique integer identifier is assigned to each neighbors of a node v to indicate its logical position. Formally, for each node v in a positional graph, there exists an injective function posv : N(v) -> {1,2,...,|N(v)|}, which assigns to each neighbor u of v a position posv(u). Note that the position of the neighbor can be implicitly used for storing useful information. For instance, let us consider the example of the region adjacency graph [see Fig. 1(b)]: posv might enumerate the neighbors of a node v, which represents the adjacent regions, following a clockwise ordering convention. The domain considered in this paper is the set D of pairs of a graph and a node, i.e., D = {(G,v) | G isin G, v isin V(G)}. We assume a supervised learning framework with the learning set L = {((G1,v1),t1),...,((Gm,vm),tm)}, where vi denotes the ith node in the set V(G) and ti is the desired target associated to vi. Finally, G = UGi and V = UV(Gi). Interestingly, all the graphs of the learning set can be combined into a unique disconnected graph, and, therefore, one might think of the learning set as the pair (G,V) where G is a graph and V a is set of pairs (vi,ti). It is worth mentioning that this compact definition is not only useful for its simplicity, but that it also captures directly the very nature of some problems where the domain consists of only one graph, for instance, a large portion of the web [see Fig. 1(c)].",       "result": "In this section, we present the experimental results, obtained on a set of simple problems carried out to study the properties of the GNN model and to prove that the method can be applied to relevant applications in relational domains. The problems that we consider, viz., the subgraph matching, the mutagenesis, and the web page ranking, have been selected since they are particularly suited to discover the properties of the model and are correlated to important real-world applications. From a practical point of view, we will see that the results obtained on some parts of mutagenesis data sets are among the best that are currently reported in the open literature (please see detailed comparison in Section IV-B). Moreover, the subgraph matching problem is relevant to several application domains. Even if the performance of our method is not comparable in terms of best accuracy on the same problem with the most efficient algorithms in the literature, the proposed approach is a very general technique that can be applied on extension of the subgraph matching problems [73]–[75]. Finally, the web page ranking is an interesting problem, since it is important in information retrieval and very few techniques have been proposed for its solution [76]. It is worth mentioning that the GNN model has been already successfully applied on larger applications, which include image classification and object localization in images [77], [78], web page ranking [79], relational learning [80], and XML classification [81].",       "conclusion": "In this paper, we introduced a novel neural network model that can handle graph inputs: the graphs can be cyclic, directed, undirected, or a mixture of these. The model is based on information diffusion and relaxation mechanisms. The approach extends into a common framework, the previous connectionist techniques for processing structured data, and the methods based on random walk models. A learning algorithm to estimate model parameters was provided and its computational complexity was studied, demonstrating that the method is suitable also for large data sets. Some promising experimental results were provided to assess the model. In particular, the results achieved on the whole Mutagenisis data set and on the unfriendly part of such a data set are the best compared with those reported in the open literature. Moreover, the experiments on the subgraph matching and on the web page ranking show that the method can be applied to problems that are related to important practical applications. The possibility of dealing with domains where the data consists of patterns and relationships gives rise to several new topics of research. For example, while in this paper it is assumed that the domain is static, it may happen that the input graphs change with time. In this case, at least two interesting issues can be considered: first, GNNs must be extended to cope with a dynamic domain; and second, no method exists, to the best of our knowledge, to model the evolution of the domain. The solution of the latter problem, for instance, may allow to model the evolution of the web and, more generally, of social networks. Another topic of future research is the study on how to deal with domains where the relationships, which are not known in advance, must be inferred. In this case, the input contains flat data and is automatically transformed into a set of graphs in order to shed some light on possible hidden relationships."   } }

Total input tokens:26385
Total output tokens:2310
Is valid JSON:True
Time elapsed: 349.66572189331055