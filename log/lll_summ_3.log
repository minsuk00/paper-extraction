{
  "filename": "3D Convolutional Neural Networks for Human Action Recognition",
  "extracted-section-list": [
    "Introduction",
    "3D Convolutional Neural Networks",
    "Related Work",
    "Experiments",
    "Conclusions and Discussions"
  ],
  "target-section-extraction-result": {
    "introduction": "Recognizing human actions in real-world environment\nfinds applications in a variety of domains including in-\ntelligent video surveillance, customer attributes, and\nshopping behavior analysis. However, accurate recog-\nnition of actions is a highly challenging task due to\ncluttered backgrounds, occlusions, and viewpoint vari-\nations, etc. Therefore, most of the existing approaches\n(Efros et al. ,2003;Sch¨ uldt et al. ,2004;Doll´ ar et al. ,\n2005;Laptev & P´ erez ,2007;Jhuang et al. ,2007)\nmake certain assumptions (e.g., small scale and view-\npoint changes) about the circumstances under which\nthe video was taken. However, such assumptions sel-\ndom hold in real-world environment. In addition, most\nof these approaches follow the conventional paradigm\nof pattern recognition, which consists of two steps in\nwhich the first step computes complex handcrafted fea-\ntures from raw video frames and the second step learns\nclassifiers based on the obtained features. In real-world\nscenarios, it is rarely known which features are impor-\ntant for the task at hand, since the choice of feature is\nhighly problem-dependent. Especially for human ac-\ntion recognition, different action classes may appear\ndramatically different in terms of their appearances\nand motion patterns.\nDeep learning models ( Fukushima ,1980;LeCun et al. ,\n1998;Hinton & Salakhutdinov ,2006;Hinton et al. ,\n2006;Bengio ,2009) are a class of machines that can\nlearn a hierarchy of features by building high-level\nfeatures from low-level ones, thereby automating the\nprocess of feature construction. Such learning ma-\nchines can be trained using either supervised or un-\nsupervised approaches, and the resulting systems have\nbeen shown to yield competitive performance in visual\nobject recognition ( LeCun et al. ,1998;Hinton et al. ,\n2006;Ranzato et al. ,2007;Lee et al. ,2009a), natu-\nral language processing ( Collobert & Weston ,2008),\nand audio classification ( Lee et al. ,2009b ) tasks. The\nconvolutional neural networks (CNNs) ( LeCun et al. ,\n1998) are a type of deep models in which trainable\nfilters and local neighborhood pooling operations are\napplied alternatingly on the raw input images, result-\ning in a hierarchy of increasingly complex features.\nIt has been shown that, when trained with appropri-3D Convolutional Neural Networks for Human Action Recognition\nate regularization ( Ahmed et al. ,2008;Yu et al. ,2008;\nMobahi et al. ,2009), CNNs can achieve superior per-\nformance on visual object recognition tasks without\nrelying on handcrafted features. In addition, CNNs\nhave been shown to be relatively insensitive to certain\nvariations on the inputs ( LeCun et al. ,2004).\nAs a class of attractive deep models for automated fea-\nture construction, CNNs have been primarily applied\non 2D images. In this paper, we consider the use of\nCNNs for human action recognition in videos. A sim-\nple approach in this direction is to treat video frames\nas still images and apply CNNs to recognize actions\nat the individual frame level. Indeed, this approach\nhas been used to analyze the videos of developing\nembryos ( Ning et al. ,2005). However, such approach\ndoes not consider the motion information encoded in\nmultiple contiguous frames. To effectively incorporate\nthe motion information in video analysis, we propose\nto perform 3D convolution in the convolutional layers\nof CNNs so that discriminative features along both\nspatial and temporal dimensions are captured. We\nshow that by applying multiple distinct convolutional\noperations at the same location on the input, multi-\nple types of features can be extracted. Based on the\nproposed 3D convolution, a variety of 3D CNN archi-\ntectures can be devised to analyze video data. We\ndevelop a 3D CNN architecture that generates multi-\nple channels of information from adjacent video frames\nand performs convolution and subsampling separately\nin each channel. The final feature representation is\nobtained by                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
stop
CompletionUsage(completion_tokens=1624, prompt_tokens=9332, total_tokens=10956)
=============================