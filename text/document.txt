Seeing through the Brain: Image Reconstruction of Visual Perception
from Human Brain Signals
Yu-Ting Lan1*, Kan Ren2, Yansen Wang2, Wei-Long Zheng1,
Dongsheng Li2, Bao-Liang Lu1, Lili Qiu2
1Shanghai Jiao Tong University,2Microsoft Research
{kanren, yansenwang }@microsoft.com, weilong@sjtu.edu.cn
Abstract
Seeing is believing , however, the underlying mechanism of
how human visual perceptions are intertwined with our cog-
nitions is still a mystery. Thanks to the recent advances
in both neuroscience and artificial intelligence, we have
been able to record the visually evoked brain activities and
mimic the visual perception ability through computational
approaches. In this paper, we pay attention to visual stimuli
reconstruction by reconstructing the observed images based
on portably accessible brain signals, i.e., electroencephalog-
raphy (EEG) data. Since EEG signals are dynamic in the
time-series format and are notorious to be noisy, processing
and extracting useful information requires more dedicated ef-
forts; In this paper, we propose a comprehensive pipeline,
named N EURO IMAGEN , for reconstructing visual stimuli im-
ages from EEG signals. Specifically, we incorporate a novel
multi-level perceptual information decoding to draw multi-
grained outputs from the given EEG data. A latent diffu-
sion model will then leverage the extracted information to
reconstruct the high-resolution visual stimuli images. The ex-
perimental results have illustrated the effectiveness of image
reconstruction and superior quantitative performance of our
proposed method.
Introduction
Understanding cortical responses to human visual percep-
tion has emerged a research hotspot, which can significantly
motivate the development of computational cognitive sys-
tem with the knowledge of neuroscience (Palazzo et al.
2020). Along with the rapid development of physiological
techniques such as functional magnetic resonance imaging
(fMRI) or electroencephalograph (EEG), it becomes possi-
ble to record the visually-evoked human brain activities for
further analysis. Thus, the research community put the atten-
tion onto these complicated brain signal data and try to re-
construct the stimuli contents used for evoking human sub-
jects in the experiments, for understanding and simulating
the human visual perception.
One of the mainstream attempts to study the human vi-
sual perception is to reconstruct the seen contents such as
images (Takagi and Nishimoto 2023) or videos (Chen, Qing,
and Zhou 2023) used to evoke the human subjective in the
*Work done during Yuting’s intern at Microsoft Research Asia,
correspondence to Kan Ren, Yansen Wang and Weilong Zheng.stimuli experiments, via computational approaches such as
deep neural networks. These works are mainly based on
fMRI data (Allen et al. 2022), while collecting these imaging
data requires expensive devices and lacks of convenience for
practical usage. In contrast, EEG has provided a more expe-
dient solution to record and analyze brain signals, yet few
works are learning visual perception upon these brain signal
data. EEG data are commonly time-series electrophysiolog-
ical signals recorded via electrodes placed upon the human
scalp, while the subjects are watching some stimuli contents
such as images which have also been temporally aligned to
the recorded signals in the data.
Though more convenient, reconstruction of visual stim-
uli from EEG signals are more challenging than that from
fMRI data. First, EEG signals are in time-series data for-
mat, which is temporal sequence and quite different to the
static 2D/3D images, leading to the challenge of the match-
ing stimuli to the corresponding brain signal pieces. Second,
the effects of electrode misplacement or body motion result
in severe artifacts in the data with quite low signal-to-noise
ratio (SNR), which have largely influenced the modeling and
understanding of the brain activities. Simply mapping the
EEG input to the pixel domain to recover the visual stim-
uli is of low quality. The existing works tackling image re-
construction from EEG either traditional generative models
from scratch (Kavasidis et al. 2017) and fine-tuning large
generative models (Bai et al. 2023), which are less efficient;
or just retrieving similar images from the data pool (Ye et al.
2022). They fail to capture semantic information or recon-
struct high-resolution outputs.
In this work, we propose a comprehensive pipeline for
Neural Image generation, namely N EURO IMAGEN , from
human brain signals. To tackle with aforementioned chal-
lenges in this task, we incorporate a multi-level seman-
tics extraction module which decodes different semantic in-
formation from the input signal with various granularity.
Specifically, the extracted information contains sample-level
semantics which is easy to decode, and pixel-level seman-
tics such as the saliency map of silhouette information that
tends to more decoding difficulties. The multi-level outputs
are further fed into the pretrained diffusion models with the
control of the generation semantics. Through this way, our
method can flexibly handle the semantic information extrac-
tion and decoding problem at different granularity and diffi-arXiv:2308.02510v2  [eess.IV]  16 Aug 2023culties, which can subsequently facilitate the generation via
effectively controlling the fixed downstream diffusion model
at different levels.
We evaluate our methods with the traditional image re-
construction solutions on EEG data. The results demon-
strate the superiority of our N EURO IMAGEN over the com-
pared methods in both quantitative and qualitative results on
the EEG-image dataset. The proposed multi-level semantics
extraction at different granularity can largely increase the
structure similarity and semantic accuracy of reconstructed
images with the observed visual stimuli.
Related Work
Diffusion Models
Recently, diffusion models have emerged as state-of-the-
art approaches in the field of generative models for sev-
eral tasks, including image synthesis, video generation, and
molecule design (Yang et al. 2022; Song, Meng, and Ermon
2020; Dhariwal and Nichol 2021). A denoising diffusion
probabilistic model (DDPM) (Ho, Jain, and Abbeel 2020;
Sohl-Dickstein et al. 2015) is a parameterized bi-directional
Markov chain using variational inference to produce sample
matching after a finite time. The forward diffusion process
is typically designed with the goal to transform any data dis-
tribution into a simple prior distribution ( e.g., an isotropic
Gaussian), and the reverse denoising diffusion process re-
verses the former by learning transition kernels parameter-
ized by deep neural networks, such as U-Net (Ronneberger,
Fischer, and Brox 2015). However, DDPM operates and un-
dergoes evaluation and optimization in pixel space, leading
to slower inference speeds and higher training costs. To ad-
dress these limitations, Rombach et al.(2022) introduced the
concept of latent diffusion models (LDMs). In LDMs, diffu-
sion models are applied within the latent space of the pow-
erful pretrained autoencoders. This approach proves to be
an effective generative model, accompanied by a separate
compression stage that selectively eliminates imperceptible
details. By operating in the latent space, LDMs overcome
the drawbacks of pixel space evaluation, enabling faster in-
ference and reducing training costs.
Image Decoding from fMRI
The most recent works reconstructing the stimuli contents
from the brain activities are mainly focused on fMRI data.
fMRI, as the measurement of the brain’s blood-oxygen-
level-dependent (BOLD) signals, has seen substantial ad-
vancements in brain signal decoding. The conventional vi-
sual decoding methods in fMRI usually rely on training
deep generative neural networks, such as generative adver-
sarial networks (GAN) and variational autoencoders (V AE)
with paired brain-image data (Shen et al. 2019; Beliy et al.
2019). However, the decoding performance of these conven-
tional methods is usually limited, and they struggle to pro-
duce visual contents with high resolution, because training
a deep generative model from scratch is in general challeng-
ing and the dataset of brain signals is relatively small and
noisy. Thus, recent research attempts to directly map brain
signals into carefully pretrained latent spaces and finetunelarge-scale pretrained models to generate diverse and high-
resolution images. Takagi and Nishimoto map the brain ac-
tivities to latent space and convert them to natural images
using LDM. MinD-Vis (Chen et al. 2023) integrates mask
brain modelings and LDM to generate more plausible im-
ages with preserved semantic information. Zeng et al. inte-
grate silhouette information from brain signals with a con-
trollable diffusion model to reconstruct high-quality images
consistent with original visual stimuli. These methods gen-
erate more plausible and semantically meaningful images.
Image Decoding from EEG Signals
EEG is more portable but has relatively lower spatial res-
olution and suffers from larger noise, compared to fMRI,
which makes decoding visual experience from brain sig-
nals a challenging problem. Brain2Image (Kavasidis et al.
2017) implements long short-term memory (LSTM) stacked
with GAN and V AE techniques to generate seen images of
ImageNet (Krizhevsky, Sutskever, and Hinton 2012) from
EEG signals (Kavasidis et al. 2017). Neurovison (Khare
et al. 2022) proposes conditional progressive growing of
GAN (CProGAN) to develop perceived images and showed
a higher inception score. Ye et al. focuses on cross-modal
alignment and retrieves images at the instance level, ensur-
ing distinguishable model output for EEG signals. We also
note that there is a parallel work DreamDiffusion (Bai et al.
2023) to ours, which finetunes the diffusion model with an
auxiliary task for aligning the EEG data and Image CLIP
embeddings. However, the end-to-end training framework of
DreamDiffusion struggles to effectively decode and utilize
multi-level semantic information from EEG signals, which
limits its ability to handle inherent noise characteristics. In
addition, DreamDiffusion requires fine-tuning the diffusion
models, which poses practical challenges and limitations in
terms of scalability and efficiency.
Methodology
In this section, we design our method, N EURO IMAGEN , to
extract multi-level semantics from EEG signals and subse-
quently integrate them into a pretrained diffusion model to
reconstruct the observed visual stimuli from EEG signals.
We briefly introduce the intuition of multi-level seman-
tics extraction in our N EURO IMAGEN . EEG signals are non-
stationary time-series signals and are easy to disturb by arti-
facts like body motion, resulting in the low SNR of the sig-
nals. To tackle this challenge, we decode different semantic
information with various granularity. Specifically, the pixel-
level semantics such as the saliency map of silhouette in-
formation preserve fine-grained color, position, and shape
details of the observed stimuli. The sample-level semantics
provides a coarse-grained description of the visual stimuli,
such the concept or category of the visual content. These
designs exhibit the capacity to effectively manage the chal-
lenges posed by noisy time-series EEG data, consequently
facilitating the reconstruction of high-quality visual stimuli.
In the following, we first formulate the problem and
give an overview of N EURO IMAGEN . Then, we describe
the multi-level semantics extractions of the N EURO IMA-EEG Signals
Visual StimuliReconstructed ImageMulti -level Information
CLIP Embedding 
Decoding From 
EEG Signals
Pixel Level 
Sample Level 
Fine-grained Control
Latent Diffusion Model
t= 0 Diffusion Process
Denoising Process
Coarse-grained Control“An image of airliner”
CLIP
Caption
CLIP EmbeddingSupervision
SupervisionpT
pM
sMldmE
ldmDFigure 1: Overview of our N EURO IMAGEN . All the modules with dotted lines, i.e.pixel-level supervision and sample-level
supervision, are only used during the training phase. and would be removed during the inference phase .
GEN, including pixel-level semantics and sample-level se-
mantics with the corresponding training details of the decod-
ing procedure. Finally, we detail the image reconstruction
procedure of N EURO IMAGEN , which integrates the coarse-
grained and fine-grained semantics with a pretrained latent
diffusion model to reconstruct the observed visual stimuli
from EEG signals.
Problem Statement
In this section, we formulate the problem and give
an overview of N EURO IMAGEN . Let the paired
{(EEG,image )}dataset as Ω = {(xi, yi)}n
i=1, where
yi∈ RH×W×3is the visual stimuli image to evoke the
brain activities and xi∈ RC×Trepresents the recorded
corresponding EEG signals. Here, Cis the channel number
of EEG sensors and Tis the temporal length of a sequence
associated with the observed image. The general objective
of this research is to reconstruct an image yusing the
corresponding EEG signals x, with a focus on achieving a
high degree of similarity to the observed visual stimuli.
Multi-level Semantics Extraction Framework
Figure 1 illustrates the architecture of N EURO IMAGEN .
In our approach, we extract multi-level semantics, repre-
sented as {M1(x), M 2(x),···, Mn(x)}, which capture var-
ious granularity ranges from coarse-grained to fine-grained
information from EEG signals corresponding to visual stim-
uli. The coarse-grained semantics serves as a high-level
overview, facilitating a quick understanding of primary at-
tributes and categories of the visual stimuli. On the other
hand, fine-grained semantics offers more detailed informa-
tion, such as localized features, subtle variations, and small-
scale patterns. The multi-level semantics are then fed into a
high-quality image reconstructing module Fto reconstruct
the visual stimuli ˆy=F[M1(x), M 2(x),···, Mn(x)].
Specifically, we give two-level semantics as follows. Let Mp
andMsbe the pixel-level semantic extractor and sample-
level semantic extractor, respectively. Pixel-level semanticsis defined as the saliency map of silhouette information
Mp(x)∈ RHp×Wp×3. This step enables us to analyze the
EEG signals in the pixel space and provide the rough struc-
ture information. Subsequently, we define the sample-level
semantics as Ms(x)∈ RL×Ds, to provide coarse-grained
information such as image category or text caption.
To fully utilize the two-level semantics, the high-quality
image reconstructing module Fis a latent diffusion model.
It begins with the saliency map Mp(x)as the initial
image and utilizes the sample-level semantics Ms(x)to
polish the saliency map and finally reconstruct ˆy=
F(Mp(x), Ms(x)).
Pixel-level Semantics Extraction
In this section, we describe how we decode the pixel-level
semantics, i.e.the saliency map of silhouette information.
The intuition of this pixel-level semantics extraction is to
capture the color, position, and shape information of the ob-
served visual stimuli, which is fine-grained and extremely
difficult to reconstruct from the noisy EEG signal. However,
as is shown in Figure 3, despite the low image resolution and
limited semantic accuracy, such a saliency map successfully
captures the rough structure information of visual stimuli
from the noisy EEG signals. Specifically, our pixel-level se-
mantics extractor Mpconsists of two components: (1) con-
trastive feature learning to obtain discriminative features of
EEG signals and (2) the estimation of the saliency map of
silhouette information based on the learned EEG features.
Contrastive Feature Learning We use contrastive learn-
ing techniques to bring together the embeddings of EEG
signals when people get similar visual stimulus, i.e. see-
ing images of the same class. The triplet loss (Schroff,
Kalenichenko, and Philbin 2015) is utilized as
Ltriplet = max(0 , β+∥fθ(xa)−fθ(xp)∥2
2
−∥fθ(xa)−fθ(xn)∥2
2),(1)
where fθis the feature extraction function (Kavasidis et al.
2017) that maps EEG signals to a feature space. xa, xp, xnare the sampled anchor, positive, and negative EEG signal
segments, respectively. The objective of eq. (1) is to mini-
mize the distance between xaandxpwith the same labels
(the class of viewed visual stimuli) while maximizing the
distance between xaandxnwith different labels. To avoid
the compression of data representations into a small cluster
by the feature extraction network, a margin term βis incor-
porated into the triplet loss.
Estimation of Saliency Map After we obtain the feature
of EEG signal fθ(x), we can now generate the saliency map
of silhouette information from it and a random sampled la-
tentz∼ N(0,1), i.e.,
Mp(x) =G(z, fθ(x)).
Gdenotes for the saliency map generator. In this paper,
we use the generator from the Generative Adversarial Net-
work(GAN) (Goodfellow et al. 2020) framework to generate
the saliency map and the adversarial loss is defined as fol-
lows:
LD
adv= max(0 ,1−D(A(y), fθ(x)))+
max(0 ,1 +D(A(Mp(x))), fθ(x))), (2)
LG
adv=−D(A(Mp(x)), fθ(x)). (3)
In GAN, besides the generator G, a discriminator Dis in-
troduced to distinguish between images from the generator
and ground truth images x. It is optimized by minimizing the
hinge loss (Lim and Ye 2017) defined in Equation (2). Ais
the differentiable augmentation function (Zhao et al. 2020).
To stabilize the adversarial training process and alleviate the
problem of mode collapse, we add the mode seeking regu-
larization (Mao et al. 2019):
Lms=−dx(G(z1, fθ(x)), G(z2, fθ(x)))
dz(z1, z2)
, (4)
where d∗(·)denotes the distance metric in image space xor
latent space zandz1, z2∼ N (0,1)are two different sam-
pled latent vectors.
To enforce the accuracy of the generated saliency map
from the visual stimuli, we use the observed image as super-
vision and incorporate the Structural Similarity Index Mea-
sure (SSIM) as well:
LSSIM= 1− 
2µxµMp(x)+C1  
2σxσMp(x)+C2

µ2x+µ2
Mp(x)+C1 
σ2x+σ2
Mp(x)+C2,
(5)
where µx,µMp(x),σx, and σMp(x)represent the mean and
standard values of the ground truth images and reconstructed
saliency maps of the generator. C1andC2are constants to
stabilize the calculation.
The final loss for the generator is the weighted sum of the
losses:
LG=α1· LG
adv+α2· Lms+α3· LSSIM, (6)
andαi∈{1,2,3}are hyperparameters to balance the loss terms.Sample-level Semantics Extraction
As aforementioned, the EEG signals are notorious for their
inherent noise, making it challenging to extract both pre-
cise and fine-grained information. Therefore, besides fine-
grained pixel-level semantics, we also involve sample-level
semantic extraction methods to derive some coarse-grained
information such as the category of the main objects of the
image content. These features have a relatively lower rank
and are easier to be aligned. Despite being less detailed,
these features can still provide accurate coarse-grained in-
formation, which is meaningful to reconstruct the observed
visual stimuli.
Specifically, the process Mswill try to align the infor-
mation decoded from the input EEG signals to some gen-
erated image captions, which are generated by some other
additional annotation model such as Contrastive Language-
Image Pretraining (CLIP) model (Radford et al. 2021). Be-
low we detail the processes of image caption ground-truth
generation and semantic decoding with alignment.
An image of 
african elephantAn elephant standing next to a 
large rock
An image of mountain bikeA man riding a mountain bike down 
a trail in the woodsAn image of
parachuteA person flying a parachute in 
the air with a banner
An image of daisyA red and white flower with yellow centerGT images Label captions BLIP captions
Figure 2: Examples of ground-truth images, label captions,
and BLIP captions, respectively.
Generation of Image Captions We propose two methods
to generate the caption for each image to help supervise the
decoding procedure of semantic information from EEG sig-
nals. Since the observed images are from ImageNet dataset
containing the class of the image, we define a straightfor-
ward and heuristic method of label caption, which utilizes
the class name of each image as the caption, as illustrated in
the middle column of Figure 2. The second method is that
we use an image caption model BLIP (Li et al. 2023), which
is a generic and computation-efficient vision-language pre-
training (VLP) model utilizing the pretrained vision modeland large language models. We opt for the default parameter
configuration of the BLIP model to caption our images. The
examples are demonstrated in the right column of Figure 2.
As can be seen, the label captions tend to focus predomi-
nantly on class-level information, and the BLIP-derived cap-
tions introduce further details on a per-image level.
Predict the Text CLIP Embedding After the generation
of the image caption ground-truth, the goal of the semantic
decoding is to extract the information from the EEG signals
to align the caption information. Note that, this procedure is
conducted in the latent space, where the latent embeddings
have been processed from the CLIP model from the above
generated captions. Specifically, We extracted the CLIP em-
beddings ˆhclip∗from the generated captions and align the
output hclipof EEG sample-level encoder with the loss func-
tion as
Lclip=||hclip−ˆhclip∗||2
2, (7)
where ∗ ∈ { B, L}denotes the BLIP caption embedding or
label caption embedding.
Combining Multi-level EEG Semantics with
Diffusion Model
In this section, we present a comprehensive explanation
of how multi-level semantics can be effectively integrated
into a diffusion model for visual stimulus reconstruction.
We utilize both pixel-level semantics, denoted as Mp(x)
(obtained using G(z, fθ(x))), and sample-level semantics,
represented as Ms(x)(obtained using hclip), to exert vari-
ous granularity control over the image reconstruction pro-
cess. The reconstructed visual stimuli are defined as ˆy=
F(Mp(x), Ms(x)) =F(G(fθ(x), hclip)).
Specifically, we used the latent diffusion model to per-
form image-to-image reconstructing with the guidance of
conditional text prompt embeddings: (1) First, we recon-
struct the pixel-level semantics G(fθ(x))from EEG sig-
nals and resize it to the resolution of observed visual stimuli
(2)G(fθ(x))is then processed by the encoder Eldmof au-
toencoder from the latent diffusion model and added noise
through the diffusion process. (3) Then, we integrate the
sample-level semantics hclipas the cross-attention input of
the U-Net to guide the denoising process. (4) We project the
output of the denoising process to image space with Dldm
and finally reconstruct the high-quality image ˆy.
Experiments
Dataset
The effectiveness of our proposed methodology is validated
using the EEG-image dataset (Spampinato et al. 2017). This
dataset is publicly accessible and consists of EEG data gath-
ered from six subjects. The data was collected by present-
ing visual stimuli to the subjects, incorporating 50 images
from 40 distinct categories within the ImageNet dataset
(Krizhevsky, Sutskever, and Hinton 2012). Each set of stim-
uli was displayed in 25-second intervals, separated by a 10-
second blackout period intended to reset the visual pathway.
This process resulted in totally 2000 images, with each ex-
periment lasting 1,400 seconds (approximately 23 minutesand 20 seconds). The EEG-image dataset encompasses a
diverse range of image classes, including animals (such as
pandas), and objects (such as airlines).
Following the common data split strategy (Kavasidis et al.
2017), we divide the pre-processed raw EEG signals and
their corresponding images into training, validation, and
testing sets, with corresponding proportions of 80% (1,600
images), 10% (200 images), and 10% (200 images) and
build one model for all subjects. The dataset is split by im-
ages, ensuring the EEG signals of all subjects in response to
a single image are not spread over splits.
Evaluation Metrics
N-way Top- kClassification Accuracy (ACC) Following
(Chen et al. 2023), we evaluate the semantic correctness of
our reconstructed images by employing the N-way top- k
classification accuracy. Specifically, the ground truth image
yand reconstructed image ˆyare fed into a pretrained Im-
ageNet1k classifier (Dosovitskiy et al. 2020), which deter-
mines whether yandˆybelong to the same class. Then we
check for the reconstructed image if the top- kclassification
inNselected classes matches the class of ground-truth im-
age. Importantly, this evaluation metric eliminates the need
for pre-defined labels for the images and serves as an indi-
cator of the semantic consistency between the ground truth
and reconstructed images. In this paper, we select 50-way
top-1 accuracy as the evaluation metric.
Inception Score (IS) IS, introduced by (Salimans et al.
2016), is commonly employed to evaluate the quality and
diversity of reconstructed images in generative models.
To compute the IS, a pretrained Inception-v3 classifier
(Szegedy et al. 2016) is utilized to calculate the class prob-
abilities for the reconstructed images. We use IS to give a
quantitative comparison between our method and baselines.
Structural Similarity Index Measure (SSIM) SSIM of-
fers a comprehensive and perceptually relevant metric for
image quality evaluation. SSIM is computed over multiple
windows of the ground truth image and the corresponding
reconstructed image in luminance, contrast, and structure
components, respectively.
Results
Experiment Results on the ImageNet Dataset
The main results are illustrated in Figure 3. The images
positioned on the left with red boxes represent the ground
truth images. The second images from the left represent the
saliency map reconstructed from EEG signals. The three im-
ages on the right exhibit the three sampling results for the
given pixel-level saliency map with the guidance of sample-
level semantics of EEG signals. Upon comparison with the
ground truth images and the reconstructed saliency maps,
we validate that our pixel-level semantics extraction from
EEG signals successfully captures the color, positional, and
shape information of viewed images, despite limited seman-
tic accuracy. Comparing the GT images and three recon-
structed samples, it is demonstrated that the latent diffu-
sion model successfully polishes the decoded saliency mapGT images Sample1 Sample2 Sample3 Saliency Map GT images Sample1 Sample2 Sample3 Saliency MapFigure 3: The main results of our N EURO IMAGEN . The images positioned on the left with red boxes represent the ground truth
images. The second images from the left represent the pixel-level saliency map reconstructed from EEG signals. The three
images on the right exhibit the three sampling results for the given saliency map under the guidance of sample-level semantics.
with coarse-grained but accurate guidance of sample-level
semantics from EEG signals. The high-quality reconstructed
images purely from brain signals are perceptually and se-
mantically similar to the viewed images.
Model ACC (%) IS SSIM
Brain2Image 5.01
NeuroVision 5.23
NEURO IMAGEN 85.6 33.50 0.249
Table 1: The quantitative results of our N EURO IMAGEN ,
Brain2Image (Kavasidis et al. 2017) and NeuroVision
(Khare et al. 2022) on EEG-image dataset.
Comparison with Baselines
The quantitative results of N EURO IMAGEN and baselines
are listed in Table 1. We have introduced the IS reported
in the relevant literature, to exemplify the quality of the
reconstructed images. The IS is calculated by encompass-
ing all images reconstructed across all subjects and all
classes within the test set. As is demonstrated in Table 1,
the IS of our N EURO IMAGEN is significantly higher than
Brain2Image and NeuroVision. Furthermore, inspired by
(Bai et al. 2023), we provide a qualitative comparison with
the baselines in Figure 4. As can be seen, the quality of the
images reconstructed by our N EURO IMAGEN is markedly
higher than those reconstructed by the Brain2Image. This
observed enhancement serves to validate the effectiveness
and superiority of our proposed methodology.Subject ACC (%) IS SSIM
subj 01 83.84 32.64 0.254
subj 02 84.26 32.33 0.247
subj 03 86.66 32.93 0.251
subj 04 86.48 32.40 0.244
subj 05 87.62 32.97 0.250
subj 06 85.25 31.76 0.245
Table 2: The quantitative results of different subjects.
Generation Consistency in Different Subjects
Since EEG signals are subject-specific cognitive processes
that differ significantly in different subjects. In this section,
we validate the robustness and feasibility of N EURO IMA-
GEN across different individuals. As is illustrated in Fig-
ure 5. The quantitative metric of different subjects are stable,
which proves the generalization ability of N EURO IMAGEN .
The qualitative results are shown in Figure 5. It can be seen
the sampling from different subjects are semantically similar
to the ground truth images.
Ablation Study
We further conduct experiments on the EEG-image dataset
to analyze the effectiveness of each module of our N EU-
ROIMAGEN . We define BandLas the sample-level seman-
tics from EEG signals using BLIP caption as supervision or
label caption as supervision. We define Ias the pixel seman-
tics from EEG signals. The effectiveness of different meth-
ods is verified by employing ACC, IS, and SSIM.Brain2Image Ours
Airliner Panda Jack -o’-LanternFigure 4: Comparison baseline Brain2Image (Kavasidis et al. 2017) and our proposed N EURO IMAGEN in three classes, namely
’Airliner’, ’Panda’, and ’Jack-o’-Lantern’. The first and second row depicts the results of Brain2Image and our N EURO IMAGEN ,
respectively.
Model B L I ACC(%) IS SSIM
1%%! 4.5 16.31 0.234
2%!% 85.9 34.12 0.180
3!%% 74.1 29.87 0.157
4!%! 65.3 25.86 0.235
5%!! 85.6 33.50 0.249
Table 3: Quantitative results of ablation studies. BandL
represent the semantic decoding using BLIP caption and la-
bel caption from EEG signals, respectively. Irepresents the
perceptual information decoding from EEG signals.
Pixel-level Semantics To demonstrate the effectiveness of
the pixel-level semantics from EEG signals, we conduct val-
idation on models 2, 3, 4, and 5. By comparing 2 with 5 and
3 with 4, we find that using the pixel-level semantics, i.e.the
saliency map, can significantly increase the structure simi-
larity of the reconstructed images and ground truth images.
Sample-level Semantics We investigate the module of
sample-level semantics decoding from EEG signals on guid-ing the denoising process. Models 1, 4, and 5 represent
the experimental results only using the saliency, both the
saliency map and sample-level semantics with the supervi-
sion of BLIP caption, and both the saliency map and sample-
level semantics with the supervision of label caption, respec-
tively. By comparing 1 with 4 and 1 with 5, the experimen-
tal results demonstrate that the use of sample-level seman-
tics significantly increases the semantic accuracy of recon-
structed images.
BLIP Captions vs Label Captions We also compare the
two caption supervision methods with models 2 with 3 and
4 with 5. The experimental results of the label caption in all
metrics are superior to using BLIP caption. We impute these
results to that the EEG signals may only capture the class-
level information. So the prediction of BLIP latent is inaccu-
rate, which decreases the performance of diffusion models.
Conclusion
In this paper, we explore to understand the visually-evoked
brain activity. Specifically, We proposed a framework,
named N EURO IMAGEN , to reconstruct images of visual
perceptions from EEG signals. The N EURO IMAGEN firstGT images Subj 01 Subj 02 Subj 03 Subj 04 Subj 06 Subj 05Anemone fish Pizza Electric guitar Cano eFigure 5: Comparison of reconstructed images on different subjects. The images on the left with red boxes represent the ground
truth images. The other six images represent the reconstructed images of different subjects. The shown classes include fish,
pizza, guitar, and canoe.
generates multi-level semantic information, i.e., pixel-level
saliency maps and sample-level textual descriptions from
EEG signals, then use the diffusion model to combine the
extracted semantics and obtain the high-resolution images.
Both qualitative and quantitative experiments reveals the
strong ability of the N EURO IMAGEN .
As a preliminary work in this area, we demonstrate the
possibility of linking human visual perceptions with compli-
cated EEG signals. We expect the findings can further moti-
vate the field of artificial intelligence, cognitive science, and
neuroscience to work together and reveal the mystery of our
brains to proceed visual perception information.
References
Allen, E. J.; St-Yves, G.; Wu, Y .; Breedlove, J. L.; Prince,
J. S.; Dowdle, L. T.; Nau, M.; Caron, B.; Pestilli, F.; Charest,
I.; et al. 2022. A massive 7T fMRI dataset to bridge cogni-
tive neuroscience and artificial intelligence. Nature Neuro-
science , 25(1): 116–126.
Bai, Y .; Wang, X.; Cao, Y .; Ge, Y .; Yuan, C.; and Shan,
Y . 2023. DreamDiffusion: Generating High-Quality Images
from Brain EEG Signals. arXiv preprint arXiv:2306.16934 .
Beliy, R.; Gaziv, G.; Hoogi, A.; Strappini, F.; Golan, T.;
and Irani, M. 2019. From voxels to pixels and back: Self-
supervision in natural-image reconstruction from fMRI. Ad-
vances in Neural Information Processing Systems , 32.Chen, Z.; Qing, J.; Xiang, T.; Yue, W. L.; and Zhou, J. H.
2023. Seeing beyond the brain: Conditional diffusion model
with sparse masked modeling for vision decoding. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , 22710–22720.
Chen, Z.; Qing, J.; and Zhou, J. H. 2023. Cinematic Mind-
scapes: High-quality Video Reconstruction from Brain Ac-
tivity. arXiv preprint arXiv:2305.11675 .
Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat
gans on image synthesis. Advances in Neural Information
Processing Systems , 34: 8780–8794.
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 .
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .
2020. Generative adversarial networks. Communications of
the ACM , 63(11): 139–144.
Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion
probabilistic models. Advances in Neural Information Pro-
cessing Systems , 33: 6840–6851.
Kavasidis, I.; Palazzo, S.; Spampinato, C.; Giordano, D.; and
Shah, M. 2017. Brain2image: Converting brain signals intoimages. In Proceedings of the 25th ACM international con-
ference on Multimedia , 1809–1817.
Khare, S.; Choubey, R. N.; Amar, L.; and Udutalapalli,
V . 2022. NeuroVision: perceived image regeneration us-
ing cProGAN. Neural Computing and Applications , 34(8):
5979–5991.
Krizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-
agenet classification with deep convolutional neural net-
works. Advances in Neural Information Processing Systems ,
25.
Li, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. Blip-2:
Bootstrapping language-image pre-training with frozen im-
age encoders and large language models. arXiv preprint
arXiv:2301.12597 .
Lim, J. H.; and Ye, J. C. 2017. Geometric gan. arXiv preprint
arXiv:1705.02894 .
Mao, Q.; Lee, H.-Y .; Tseng, H.-Y .; Ma, S.; and Yang, M.-
H. 2019. Mode seeking generative adversarial networks for
diverse image synthesis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
1429–1437.
Palazzo, S.; Spampinato, C.; Kavasidis, I.; Giordano, D.;
Schmidt, J.; and Shah, M. 2020. Decoding brain represen-
tations by multimodal learning of neural activity and visual
features. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 43(11): 3833–3849.
Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
et al. 2021. Learning transferable visual models from nat-
ural language supervision. In International Conference on
Machine Learning , 8748–8763.
Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-
mer, B. 2022. High-resolution image synthesis with latent
diffusion models. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 10684–
10695.
Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Con-
volutional networks for biomedical image segmentation. In
Medical Image Computing and Computer-Assisted Interven-
tion, 234–241.
Salimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V .; Rad-
ford, A.; and Chen, X. 2016. Improved techniques for train-
ing gans. Advances in Neural Information Processing Sys-
tems, 29.
Schroff, F.; Kalenichenko, D.; and Philbin, J. 2015. Facenet:
A unified embedding for face recognition and clustering. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , 815–823.
Shen, G.; Dwivedi, K.; Majima, K.; Horikawa, T.; and
Kamitani, Y . 2019. End-to-end deep image reconstruction
from human brain activity. Frontiers in Computational Neu-
roscience , 13: 21.
Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and
Ganguli, S. 2015. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , 2256–2265.Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion
implicit models. arXiv preprint arXiv:2010.02502 .
Spampinato, C.; Palazzo, S.; Kavasidis, I.; Giordano, D.;
Souly, N.; and Shah, M. 2017. Deep learning human mind
for automated visual classification. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , 6809–6817.
Szegedy, C.; Vanhoucke, V .; Ioffe, S.; Shlens, J.; and Wo-
jna, Z. 2016. Rethinking the inception architecture for com-
puter vision. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2818–2826.
Takagi, Y .; and Nishimoto, S. 2023. High-resolution im-
age reconstruction with latent diffusion models from human
brain activity. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 14453–14463.
Yang, L.; Zhang, Z.; Song, Y .; Hong, S.; Xu, R.; Zhao, Y .;
Shao, Y .; Zhang, W.; Cui, B.; and Yang, M.-H. 2022. Dif-
fusion models: A comprehensive survey of methods and ap-
plications. arXiv preprint arXiv:2209.00796 .
Ye, Z.; Yao, L.; Zhang, Y .; and Gustin, S. 2022.
See what you see: Self-supervised cross-modal retrieval
of visual stimuli from brain activity. arXiv preprint
arXiv:2208.03666 .
Zeng, B.; Li, S.; Liu, X.; Gao, S.; Jiang, X.; Tang, X.; Hu,
Y .; Liu, J.; and Zhang, B. 2023. Controllable Mind Visual
Diffusion Model. arXiv preprint arXiv:2305.10135 .
Zhao, S.; Liu, Z.; Lin, J.; Zhu, J.-Y .; and Han, S. 2020.
Differentiable augmentation for data-efficient gan training.
Advances in Neural Information Processing Systems , 33:
7559–7570.